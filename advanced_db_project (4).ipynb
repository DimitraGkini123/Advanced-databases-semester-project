{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff2a9a-2d51-4fea-b9d0-d204736ddcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d17c5fe-055a-4b17-973a-6dadeb88b914",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>27</td><td>application_1738075734771_0028</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0028/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-104.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0028_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4', 'spark.executor.memory': '1g', 'spark.executor.cores': '1', 'spark.driver.memory': '2g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>11</td><td>application_1738075734771_0012</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0012/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-100.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0012_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>14</td><td>application_1738075734771_0015</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0015/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-139.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0015_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>15</td><td>application_1738075734771_0016</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0016/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0016_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>18</td><td>application_1738075734771_0019</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0019/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-137.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0019_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>21</td><td>application_1738075734771_0022</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0022/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-72.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0022_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>22</td><td>application_1738075734771_0023</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0023/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0023_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>26</td><td>application_1738075734771_0027</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0027/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-137.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0027_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>27</td><td>application_1738075734771_0028</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0028/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-104.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0028_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.memory\": \"1g\",\n",
    "        \"spark.executor.cores\": \"1\",\n",
    "        \"spark.driver.memory\": \"2g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f866a291-0a25-4771-a84e-6f1d31e92c68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the Kids: 15923\n",
      "Number of rows in the Young Adults: 33605\n",
      "Number of rows in the Adults: 121093\n",
      "Number of rows in the Eldery: 5985\n",
      "Time taken: 50.12 seconds"
     ]
    }
   ],
   "source": [
    "\n",
    "# Spark RDD code\n",
    "from pyspark.sql import SparkSession\n",
    "#To log our application's execution time:\n",
    "import time\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RDD query 1 execution\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#crime data\n",
    "#When using the given format the csv file was wrongly separated and it gave wrong records. That's why we created a function that is able to open the csv with respect to \n",
    "#commas inside columns.\n",
    "\n",
    "#cd_2010_2019 = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\") \\\n",
    "#                   .map(lambda x: (x.split(\",\")))\n",
    "#cd_2020_ = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\") \\\n",
    "#                    .map(lambda x: (x.split(\",\")))\n",
    "\n",
    "def parse_csv(line):\n",
    "    return next(csv.reader(StringIO(line)))\n",
    "\n",
    "cd_2010_2019 = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\") \\\n",
    "    .map(parse_csv)\n",
    "\n",
    "cd_2020_ = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\") \\\n",
    "    .map(parse_csv)\n",
    "\n",
    "header_2010_2019 = cd_2010_2019.first()\n",
    "header_2020 = cd_2020_.first()\n",
    "\n",
    "data_2010_2019 = cd_2010_2019.filter(lambda x: x != header_2010_2019)\n",
    "data_2020 = cd_2020_.filter(lambda x: x != header_2020)\n",
    "\n",
    "#Filter to get only the incidents with aggravated assault\n",
    "cd_aggr_2010_2019 = data_2010_2019.filter(lambda x: \"AGGRAVATED ASSAULT\" in x[9])\n",
    "cd_aggr_2020_ = data_2020.filter(lambda x: \"AGGRAVATED ASSAULT\" in x[9])\n",
    "\n",
    "#For each category, adjust the age limits, then combine the data from both files and finally sort them on descending order by Victim Age\n",
    "cd_kids_2010_2019 = cd_aggr_2010_2019.filter(lambda x: int(x[11]) < 18 and int(x[11]) >= 0)\n",
    "cd_kids_2020_ = cd_aggr_2020_.filter(lambda x: int(x[11]) < 18 and int(x[11]) >= 0)\n",
    "cd_kids = cd_kids_2010_2019.union(cd_kids_2020_)\n",
    "sorted_cd_kids = cd_kids.sortBy(lambda x: x[11], ascending=False)\n",
    "\n",
    "cd_young_adlt_2010_2019 = cd_aggr_2010_2019.filter(lambda x: int(x[11]) >= 18 and int(x[11]) < 25)\n",
    "cd_young_adlt_2020_ = cd_aggr_2020_.filter(lambda x: int(x[11]) >= 18 and int(x[11]) < 25)\n",
    "cd_young_adlt = cd_young_adlt_2010_2019.union(cd_young_adlt_2020_)\n",
    "sorted_cd_young_adlt = cd_young_adlt.sortBy(lambda x: x[11], ascending=False)\n",
    "\n",
    "cd_adlt_2010_2019 = cd_aggr_2010_2019.filter(lambda x: int(x[11]) >= 25 and int(x[11]) <= 64)\n",
    "cd_adlt_2020_ = cd_aggr_2020_.filter(lambda x: int(x[11]) >= 25 and int(x[11]) <= 64)\n",
    "cd_adlt = cd_adlt_2010_2019.union(cd_adlt_2020_)\n",
    "sorted_cd_adlt = cd_adlt.sortBy(lambda x: x[11], ascending=False)\n",
    "\n",
    "cd_eldery_2010_2019 = cd_aggr_2010_2019.filter(lambda x: int(x[11]) > 64)\n",
    "cd_eldery_2020_ = cd_aggr_2020_.filter(lambda x: int(x[11]) > 64)\n",
    "cd_eldery = cd_eldery_2010_2019.union(cd_eldery_2020_)\n",
    "sorted_cd_eldery = cd_eldery.sortBy(lambda x: x[11], ascending=False)\n",
    "\n",
    "end_time = time.time()\n",
    "ex_time=end_time - start_time\n",
    "\n",
    "# Take a sample of each RDD\n",
    "#print(\"Sample of sorted_cd_kids:\")\n",
    "#print(sorted_cd_kids.take(10))\n",
    "\n",
    "#print(\"Sample of sorted_cd_young_adlt:\")\n",
    "#print(sorted_cd_young_adlt.take(10))\n",
    "\n",
    "#print(\"Sample of sorted_cd_adlt:\")\n",
    "#print(sorted_cd_adlt.take(10))\n",
    "\n",
    "#print(\"Sample of sorted_cd_eldery:\")\n",
    "\n",
    "#print(sorted_cd_eldery.take(10))\n",
    "\n",
    "print(f\"Number of rows in the Kids: {sorted_cd_kids.count()}\")\n",
    "print(f\"Number of rows in the Young Adults: {sorted_cd_young_adlt.count()}\")\n",
    "print(f\"Number of rows in the Adults: {sorted_cd_adlt.count()}\")\n",
    "print(f\"Number of rows in the Eldery: {sorted_cd_eldery.count()}\")\n",
    "\n",
    "print(f\"Time taken: {ex_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16e259a7-98f8-4c37-af58-6e06d492f462",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the Kids: 15923\n",
      "Number of rows in the Young Adults: 33605\n",
      "Number of rows in the Adults: 121093\n",
      "Number of rows in the Eldery: 5985\n",
      "Time taken: 2.34 seconds"
     ]
    }
   ],
   "source": [
    "#Query 1\n",
    "\n",
    "# Spark DataFrame code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF query 1 execution\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "crimedata = StructType([\n",
    "    StructField(\"DR_NO\", StringType(), True),\n",
    "    StructField(\"Date Rptd\", StringType(), True),\n",
    "    StructField(\"DATE OCC\", StringType(), True),\n",
    "    StructField(\"TIME OCC\", StringType(), True),\n",
    "    StructField(\"AREA\", IntegerType(), True),\n",
    "    StructField(\"AREA NAME\", StringType(), True),\n",
    "    StructField(\"Rpt Dist No\", IntegerType(), True),\n",
    "    StructField(\"Part 1-2\", IntegerType(), True),\n",
    "    StructField(\"Crm Cd\", IntegerType(), True),\n",
    "    StructField(\"Crm Cd Desc\", StringType(), True),\n",
    "    StructField(\"Mocodes\", StringType(), True),\n",
    "    StructField(\"Vict Age\", IntegerType(), True),\n",
    "    StructField(\"Vict Sex\", StringType(), True),\n",
    "    StructField(\"Vict Descent\", StringType(), True),\n",
    "    StructField(\"Premis Cd\", IntegerType(), True),\n",
    "    StructField(\"Premis Desc\", StringType(), True),\n",
    "    StructField(\"Weapon Used Cd\", IntegerType(), True),\n",
    "    StructField(\"Weapon Desc\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Status Desc\", StringType(), True),\n",
    "    StructField(\"Crm Cd 1\", IntegerType(), True),\n",
    "    StructField(\"Crm Cd 2\", IntegerType(), True),\n",
    "    StructField(\"Crm Cd 3\", IntegerType(), True),\n",
    "    StructField(\"Crm Cd 4\", IntegerType(), True),\n",
    "    StructField(\"LOCATION\", StringType(), True),\n",
    "    StructField(\"Cross Street\", StringType(), True),\n",
    "    StructField(\"LAT\", FloatType(), True),\n",
    "    StructField(\"LON\", FloatType(), True)\n",
    "])\n",
    "\n",
    "schema1 = \"Crm_Cd_Desc STRING, Vict_Age INT\"\n",
    "\n",
    "# Starting Time\n",
    "start_time = time.time()\n",
    "\n",
    "crimedata_2010_2019 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=False, schema=crimedata)\n",
    "crimedata_2020_ = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", header=False, schema=crimedata)\n",
    "\n",
    "#Filter to get only the incidents with aggravated assault\n",
    "crimedata_2010_2019_aggr = crimedata_2010_2019.filter(col(\"Crm Cd Desc\").contains(\"AGGRAVATED ASSAULT\"))\n",
    "crimedata_2020_aggr = crimedata_2020_.filter(col(\"Crm Cd Desc\").contains(\"AGGRAVATED ASSAULT\"))\n",
    "\n",
    "#For each category, adjust the age limits, then combine the data from both files and finally sort them on descending order by Victim Age\n",
    "kids2010_2019 = crimedata_2010_2019_aggr.filter((col(\"Vict Age\") >= 0) & (col(\"Vict Age\") < 18))\n",
    "kids2020_ = crimedata_2020_aggr.filter((col(\"Vict Age\") >= 0) & (col(\"Vict Age\") < 18))\n",
    "kids = kids2010_2019.union(kids2020_)\n",
    "sorted_kids = kids.sort(col(\"Vict Age\"), ascending=False)\n",
    "sorted_kids = kids.orderBy(col(\"Vict Age\").desc())\n",
    "\n",
    "young_adult2010_2019 = crimedata_2010_2019_aggr.filter((col(\"Vict Age\") >= 18) & (col(\"Vict Age\") <= 24))\n",
    "young_adult2020_ = crimedata_2020_aggr.filter((col(\"Vict Age\") >= 18) & (col(\"Vict Age\") <= 24))\n",
    "young_adults = young_adult2010_2019.union(young_adult2020_)\n",
    "sorted_young_adults = young_adults.orderBy(col(\"Vict Age\").desc())\n",
    "\n",
    "adult2010_2019 = crimedata_2010_2019_aggr.filter((col(\"Vict Age\") >= 25) & (col(\"Vict Age\") <= 64))\n",
    "adult2020_ = crimedata_2020_aggr.filter((col(\"Vict Age\") >= 25) & (col(\"Vict Age\") <= 64))\n",
    "adults = adult2010_2019.union(adult2020_)\n",
    "sorted_adults = adults.orderBy(col(\"Vict Age\").desc())\n",
    "\n",
    "elderly2010_2019 = crimedata_2010_2019_aggr.filter(col(\"Vict Age\") > 64)\n",
    "elderly2020_ = crimedata_2020_aggr.filter(col(\"Vict Age\") > 64)\n",
    "elderly = elderly2010_2019.union(elderly2020_)\n",
    "sorted_elderly = elderly.orderBy(col(\"Vict Age\").desc())\n",
    "\n",
    "#Show how much time did it take\n",
    "end_time = time.time()\n",
    "ex_time = end_time - start_time\n",
    "\n",
    "#Show results\n",
    "#sorted_kids.show(10)\n",
    "#sorted_young_adults.show(10)\n",
    "#sorted_adults.show(10)\n",
    "#sorted_elderly.show(10)\n",
    "\n",
    "print(f\"Number of rows in the Kids: {sorted_kids.count()}\")\n",
    "print(f\"Number of rows in the Young Adults: {sorted_young_adults.count()}\")\n",
    "print(f\"Number of rows in the Adults: {sorted_adults.count()}\")\n",
    "print(f\"Number of rows in the Eldery: {sorted_elderly.count()}\")\n",
    "print(f\"Time taken: {ex_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1bcc3470-e89d-44ca-9928-97cc981a92ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>92</td><td>application_1738075734771_0093</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0093/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-32.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0093_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4', 'spark.executor.memory': '2g', 'spark.executor.cores': '4', 'spark.driver.memory': '2g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>83</td><td>application_1738075734771_0084</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0084/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-104.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0084_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>86</td><td>application_1738075734771_0087</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0087/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-139.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0087_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>88</td><td>application_1738075734771_0089</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0089/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-138.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0089_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>89</td><td>application_1738075734771_0090</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0090/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-86.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0090_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>90</td><td>application_1738075734771_0091</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0091/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-75.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0091_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>91</td><td>application_1738075734771_0092</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0092/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-104.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0092_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>92</td><td>application_1738075734771_0093</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0093/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-32.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0093_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"4\",\n",
    "        \"spark.driver.memory\": \"2g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a193d26-8afa-48a6-8816-29d1fd9b97ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of sorted_cd_young_adlt:\n",
      "[['180200700', '03/31/2018 12:00:00 AM', '03/31/2018 12:00:00 AM', '0330', '02', 'Rampart', '0204', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '0417 0416 0443 2000 1813 1310 0429 1206', '24', 'F', 'H', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AA', 'Adult Arrest', '236', '', '', '', '700    IMOGEN                       AV', '', '34.0822', '-118.2832'], ['181508373', '03/26/2018 12:00:00 AM', '03/25/2018 12:00:00 AM', '0340', '15', 'N Hollywood', '1532', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 1814 0444', '24', 'F', 'H', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AA', 'Adult Arrest', '236', '', '', '', '6100    WHITSETT                     AV', '', '34.1813', '-118.4052'], ['180325257', '11/11/2018 12:00:00 AM', '11/11/2018 12:00:00 AM', '1400', '03', 'Southwest', '0376', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 0416', '24', 'F', 'B', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '312', 'PIPE/METAL PIPE', 'IC', 'Invest Cont', '236', '', '', '', '3900    DENKER                       AV', '', '34.0149', '-118.3045'], ['182110245', '05/10/2018 12:00:00 AM', '05/10/2018 12:00:00 AM', '1100', '21', 'Topanga', '2118', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '0408 1402 0913 2000', '24', 'F', 'H', '501', 'SINGLE FAMILY DWELLING', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AA', 'Adult Arrest', '236', '998', '', '', '20400    ROSCOE                       BL', '', '34.2201', '-118.5776'], ['180919789', '10/25/2018 12:00:00 AM', '10/25/2018 12:00:00 AM', '2140', '09', 'Van Nuys', '0969', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '0444 0416 1814 2000 0446 1300 1414', '24', 'M', 'W', '101', 'STREET', '307', 'VEHICLE', 'AO', 'Adult Other', '236', '', '', '', 'COLDWATER CA                 AV', 'ADDISON                      ST', '34.162', '-118.4166'], ['181308804', '03/26/2018 12:00:00 AM', '03/25/2018 12:00:00 AM', '2147', '13', 'Newton', '1343', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 0913 0432 0416 0408', '24', 'F', 'B', '104', 'DRIVEWAY', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'IC', 'Invest Cont', '236', '', '', '', '600 E  37TH                         ST', '', '34.0128', '-118.2653'], ['181515498', '08/01/2018 12:00:00 AM', '07/31/2018 12:00:00 AM', '1800', '15', 'N Hollywood', '1532', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '0913 2000 1814 0334 0421 0416 1307 0329 0449', '24', 'F', 'B', '101', 'STREET', '204', 'FOLDING KNIFE', 'AO', 'Adult Other', '236', '', '', '', 'VICTORY', 'WHITSETT', '34.1867', '-118.4052'], ['181822391', '11/14/2018 12:00:00 AM', '11/14/2018 12:00:00 AM', '1400', '18', 'Southeast', '1861', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 0400 1813 0416', '24', 'F', 'B', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '308', 'STICK', 'AO', 'Adult Other', '236', '', '', '', '11800 S  FIGUEROA                     ST', '', '33.9274', '-118.2871'], ['181919220', '10/17/2018 12:00:00 AM', '10/17/2018 12:00:00 AM', '1950', '19', 'Mission', '1966', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '1813 1814 2000 0448 0319 0444 0408 0443 0419', '24', 'F', 'H', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AA', 'Adult Arrest', '236', '', '', '', '9200    VAN NUYS                     BL', '', '34.2373', '-118.4502'], ['181017686', '10/25/2018 12:00:00 AM', '10/19/2018 12:00:00 AM', '0200', '10', 'West Valley', '1079', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 0400 0444', '24', 'F', 'W', '101', 'STREET', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AO', 'Adult Other', '236', '', '', '', 'MAGNOLIA                     AV', 'DENSMORE                     ST', '34.1649', '-118.4771']]\n",
      "Sample of sorted_cd_adlt:\n",
      "[['131805923', '02/12/2013 12:00:00 AM', '02/11/2013 12:00:00 AM', '0800', '18', 'Southeast', '1822', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '0400 1202 1813 0416 2000', '64', 'F', 'B', '501', 'SINGLE FAMILY DWELLING', '304', 'CLUB/BAT', 'IC', 'Invest Cont', '236', '', '', '', '10100 S  BROADWAY', '', '33.9447', '-118.2781'], ['131115805', '08/15/2013 12:00:00 AM', '08/15/2013 12:00:00 AM', '1200', '11', 'Northeast', '1163', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '0416 2000', '64', 'M', 'H', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '500', 'UNKNOWN WEAPON/OTHER WEAPON', 'AO', 'Adult Other', '236', '', '', '', '1200 N  BERENDO                      ST', '', '34.0934', '-118.2947'], ['130909746', '04/28/2013 12:00:00 AM', '04/28/2013 12:00:00 AM', '1910', '09', 'Van Nuys', '0914', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 1241 0416 0447', '64', 'F', 'H', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '500', 'UNKNOWN WEAPON/OTHER WEAPON', 'AA', 'Adult Arrest', '236', '', '', '', '15000    VANOWEN                      ST', '', '34.1939', '-118.4596'], ['151220881', '09/05/2015 12:00:00 AM', '09/05/2015 12:00:00 AM', '0340', '12', '77th Street', '1252', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 0400 0444 1202', '64', 'M', 'B', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '205', 'KITCHEN KNIFE', 'AO', 'Adult Other', '236', '', '', '', '2300 W  FLORENCE                     AV', '', '33.9746', '-118.3187'], ['150518161', '11/16/2015 12:00:00 AM', '11/16/2015 12:00:00 AM', '1630', '05', 'Harbor', '0565', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '1814 0416 1202 2000', '64', 'F', 'W', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AO', 'Adult Other', '236', '', '', '', '500 W  8TH                          ST', '', '33.737', '-118.2879'], ['220314523', '08/01/2022 12:00:00 AM', '06/25/2022 12:00:00 AM', '0030', '03', 'Southwest', '0362', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 0913 1814 0444 0408 2034', '64', 'F', 'H', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'IC', 'Invest Cont', '236', '', '', '', '4600    COLISEUM                     ST', '', '34.0169', '-118.3472'], ['221804877', '01/20/2022 12:00:00 AM', '01/20/2022 12:00:00 AM', '0130', '18', 'Southeast', '1804', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '1202 2000 1813 0913 1821 0416 0432 0445 0400', '64', 'M', 'B', '501', 'SINGLE FAMILY DWELLING', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AA', 'Adult Arrest', '236', '', '', '', '8800 S  CENTRAL                      AV', '', '33.9574', '-118.2564'], ['231807526', '03/11/2023 12:00:00 AM', '03/11/2023 12:00:00 AM', '0001', '18', 'Southeast', '1801', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 0913 1813 0416 2021 1202', '64', 'M', 'B', '501', 'SINGLE FAMILY DWELLING', '219', 'SCREWDRIVER', 'AO', 'Adult Other', '236', '', '', '', '500 W  93RD                         ST', '', '33.9519', '-118.2827'], ['231804447', '01/07/2023 12:00:00 AM', '01/07/2023 12:00:00 AM', '0430', '18', 'Southeast', '1849', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 0913 1814 1202 0416', '64', 'M', 'B', '501', 'SINGLE FAMILY DWELLING', '500', 'UNKNOWN WEAPON/OTHER WEAPON', 'AO', 'Adult Other', '236', '', '', '', '2300    SANTA ANA BL', '', '33.9347', '-118.2319'], ['230507929', '04/19/2023 12:00:00 AM', '04/19/2023 12:00:00 AM', '0630', '05', 'Harbor', '0515', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 0913 0445 2021 1202 0416 1821', '64', 'M', 'H', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '500', 'UNKNOWN WEAPON/OTHER WEAPON', 'AA', 'Adult Arrest', '236', '', '', '', '1400 N  MARINE                       AV', '', '33.7911', '-118.2646']]\n",
      "Sample of sorted_cd_eldery:\n",
      "[['240105245', '01/18/2024 12:00:00 AM', '01/18/2024 12:00:00 AM', '1900', '01', 'Central', '0163', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '1822 2004 1266 2003 0447 0400', '99', 'M', 'H', '710', 'OTHER PREMISE', '212', 'BOTTLE', 'IC', 'Invest Cont', '236', '', '', '', '200 W  7TH                          ST', '', '34.0445', '-118.2523'], ['180713962', '07/24/2018 12:00:00 AM', '07/22/2018 12:00:00 AM', '0900', '07', 'Wilshire', '0745', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 1814 0416 0446 0408', '99', 'F', 'B', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AA', 'Adult Arrest', '236', '', '', '', '1000    MEADOWBROOK                  AV', '', '34.0573', '-118.3468'], ['190323894', '11/03/2019 12:00:00 AM', '11/03/2019 12:00:00 AM', '0800', '03', 'Southwest', '0393', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 1814 0429 0416 0329 2007 1222 0447', '99', 'F', 'B', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'IC', 'Invest Cont', '236', '', '', '', '4200    LEIMERT                      BL', '', '34.0082', '-118.3263'], ['110210817', '05/16/2011 12:00:00 AM', '05/11/2011 12:00:00 AM', '1000', '02', 'Rampart', '0281', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '0408 0416 1241 1259 1813 2000', '99', 'F', 'H', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AA', 'Adult Arrest', '236', '', '', '', '1200 S  ALVARADO                     ST', '', '34.0488', '-118.2817'], ['180121778', '08/20/2018 12:00:00 AM', '08/20/2018 12:00:00 AM', '0700', '01', 'Central', '0192', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2004 2000 0416 0408 1813 1814', '99', 'F', 'B', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AO', 'Adult Other', '236', '', '', '', '1600 S  HOPE                         ST', '', '34.0363', '-118.2677'], ['181805391', '01/26/2018 12:00:00 AM', '01/25/2018 12:00:00 AM', '2320', '18', 'Southeast', '1835', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 0416 1814', '99', 'F', 'B', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AA', 'Adult Arrest', '236', '', '', '', '10400 S  CENTRAL                      AV', '', '33.9419', '-118.2541'], ['180514402', '08/12/2018 12:00:00 AM', '08/12/2018 12:00:00 AM', '2120', '05', 'Harbor', '0566', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '0913 0448 1814 2000 1300', '99', 'F', 'B', '101', 'STREET', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AO', 'Adult Other', '236', '350', '', '', 'BEACON                       ST', '12TH                         ST', '33.7333', '-118.2805'], ['221607836', '05/02/2022 12:00:00 AM', '05/01/2022 12:00:00 AM', '2325', '16', 'Foothill', '1676', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 1814 0416 0913', '99', 'F', 'H', '101', 'STREET', '307', 'VEHICLE', 'AO', 'Adult Other', '236', '', '', '', 'SUNLAND', 'GLENOAKS', '34.2299', '-118.3665'], ['220110805', '04/21/2022 12:00:00 AM', '04/07/2022 12:00:00 AM', '2130', '01', 'Central', '0153', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 0416 0408 1813 0913', '99', 'F', 'H', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'IC', 'Invest Cont', '236', '', '', '', '200 W  7TH                          ST', '', '34.0467', '-118.252'], ['231812692', '06/29/2023 12:00:00 AM', '06/29/2023 12:00:00 AM', '2020', '18', 'Southeast', '1841', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 0913 1813 0408 0400', '99', 'F', 'H', '501', 'SINGLE FAMILY DWELLING', '304', 'CLUB/BAT', 'AO', 'Adult Other', '236', '', '', '', '11000 S  HOOVER                       ST', '', '33.9356', '-118.287']]\n",
      "Sample of sorted_cd_kids:\n",
      "[['161715887', '09/02/2016 12:00:00 AM', '09/02/2016 12:00:00 AM', '2230', '17', 'Devonshire', '1728', '1', '235', 'CHILD ABUSE (PHYSICAL) - AGGRAVATED ASSAULT', '0400 0416 0552 1259', '17', 'M', 'O', '501', 'SINGLE FAMILY DWELLING', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AO', 'Adult Other', '235', '', '', '', '18200    BERMUDA                      ST', '', '34.2663', '-118.5296'], ['161907676', '03/06/2016 12:00:00 AM', '03/05/2016 12:00:00 AM', '1640', '19', 'Mission', '1943', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '1814 0406 0429 1259 0356 2000 0416 1813 0444', '17', 'F', 'H', '102', 'SIDEWALK', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AA', 'Adult Arrest', '236', '', '', '', 'FOX', 'ARLETA', '34.267', '-118.4525'], ['160309102', '03/20/2016 12:00:00 AM', '03/19/2016 12:00:00 AM', '1615', '03', 'Southwest', '0355', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 1813 0408 0416 0446 0429 0448', '17', 'F', 'H', '505', 'MOTEL', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AO', 'Adult Other', '236', '', '', '', '3500 S  WESTERN                      AV', '', '34.0219', '-118.3133'], ['161205730', '01/26/2016 12:00:00 AM', '01/26/2016 12:00:00 AM', '0900', '12', '77th Street', '1235', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 1823 1259 1251 1309 1240 0411 0334', '17', 'F', 'H', '101', 'STREET', '204', 'FOLDING KNIFE', 'JA', 'Juv Arrest', '236', '', '', '', 'HALLDALE                     AV', '62ND                         ST', '33.9833', '-118.3024'], ['161816800', '08/26/2016 12:00:00 AM', '08/26/2016 12:00:00 AM', '1855', '18', 'Southeast', '1844', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '0302 0913 0334 1402 2000 1814', '17', 'F', 'B', '501', 'SINGLE FAMILY DWELLING', '114', 'AIR PISTOL/REVOLVER/RIFLE/BB GUN', 'AA', 'Adult Arrest', '236', '', '', '', '300 E  113TH                        ST', '', '33.932', '-118.2695'], ['160620021', '09/15/2016 12:00:00 AM', '09/15/2016 12:00:00 AM', '1615', '06', 'Hollywood', '0637', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '0416 1814 0913 2000', '17', 'F', 'H', '101', 'STREET', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AA', 'Adult Arrest', '236', '', '', '', 'VISTA DEL MAR', 'YUCCA', '34.1038', '-118.3238'], ['171811886', '05/21/2017 12:00:00 AM', '05/21/2017 12:00:00 AM', '1315', '18', 'Southeast', '1837', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '1814 2000 0334 0448 0408 0302 0400 1309 0906 1312', '17', 'F', 'B', '101', 'STREET', '103', 'RIFLE', 'AA', 'Adult Arrest', '236', '', '', '', '100TH                        ST', 'SUCCESS                      AV', '33.9363', '-118.2426'], ['170312368', '05/06/2017 12:00:00 AM', '05/06/2017 12:00:00 AM', '0215', '03', 'Southwest', '0362', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '2000 1814 0416', '17', 'F', 'B', '502', '\"MULTI-UNIT DWELLING (APARTMENT', ' DUPLEX', ' ETC)\"', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'AA', 'Adult Arrest', '236', '', '', '', '3900    COCO                         AV', '', '34.0155', '-118.3497'], ['181817989', '08/31/2018 12:00:00 AM', '08/30/2017 12:00:00 AM', '1030', '18', 'Southeast', '1844', '1', '236', 'INTIMATE PARTNER - AGGRAVATED ASSAULT', '1276 0400 0408 2000', '17', 'F', 'H', '501', 'SINGLE FAMILY DWELLING', '400', '\"STRONG-ARM (HANDS', ' FIST', ' FEET OR BODILY FORCE)\"', 'IC', 'Invest Cont', '236', '', '', '', '800 E  112TH                        ST', '', '33.9328', '-118.2603'], ['171300638', '04/30/2017 12:00:00 AM', '04/30/2017 12:00:00 AM', '2000', '13', 'Newton', '1385', '1', '235', 'CHILD ABUSE (PHYSICAL) - AGGRAVATED ASSAULT', '1251 0416 0553 1259 0334', '17', 'M', 'H', '501', 'SINGLE FAMILY DWELLING', '308', 'STICK', 'AO', 'Adult Other', '235', '', '', '', '5900    AVALON                       BL', '', '33.9874', '-118.2652']]\n",
      "42.66136574745178"
     ]
    }
   ],
   "source": [
    "# Spark RDD code\n",
    "from pyspark.sql import SparkSession\n",
    "# To log our application's execution time:\n",
    "import time\n",
    "\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RDD query 1 execution\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext\n",
    "\n",
    "start_time = time.time()\n",
    "#crime data\n",
    "cd_2010_2019 = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\") \\\n",
    "                    .map(lambda x: (x.split(\",\")))\n",
    "cd_2020_ = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\") \\\n",
    "                    .map(lambda x: (x.split(\",\")))\n",
    "\n",
    "header_2010_2019 = cd_2010_2019.first()\n",
    "header_2020 = cd_2020_.first()\n",
    "\n",
    "data_2010_2019 = cd_2010_2019.filter(lambda x: x != header_2010_2019)\n",
    "data_2020 = cd_2020_.filter(lambda x: x != header_2020)\n",
    "\n",
    "cd_aggr_2010_2019 = data_2010_2019.filter(lambda x: \"AGGRAVATED ASSAULT\" in x[9])\n",
    "cd_aggr_2020_ = data_2020.filter(lambda x: \"AGGRAVATED ASSAULT\" in x[9])\n",
    "\n",
    "cd_kids_2010_2019 = cd_aggr_2010_2019.filter(lambda x: x[11] < \"18\" and x[11] >= \"0\")\n",
    "cd_kids_2020_ = cd_aggr_2020_.filter(lambda x: x[11] < \"18\" and x[11] > \"0\")\n",
    "cd_kids = cd_kids_2010_2019.union(cd_kids_2020_)\n",
    "sorted_cd_kids = cd_kids.sortBy(lambda x: x[11], ascending=False)\n",
    "\n",
    "cd_young_adlt_2010_2019 = cd_aggr_2010_2019.filter(lambda x: int(x[11]) >= 18 and int(x[11]) < 25)\n",
    "cd_young_adlt_2020_ = cd_aggr_2020_.filter(lambda x: int(x[11]) >= 18 and int(x[11]) < 25)\n",
    "cd_young_adlt = cd_young_adlt_2010_2019.union(cd_young_adlt_2020_)\n",
    "sorted_cd_young_adlt = cd_young_adlt.sortBy(lambda x: x[11], ascending=False)\n",
    "\n",
    "cd_adlt_2010_2019 = cd_aggr_2010_2019.filter(lambda x: x[11] >= \"25\" and x[11] <= \"64\")\n",
    "cd_adlt_2020_ = cd_aggr_2020_.filter(lambda x: x[11] >= \"25\" and x[11] <= \"64\")\n",
    "cd_adlt = cd_adlt_2010_2019.union(cd_adlt_2020_)\n",
    "sorted_cd_adlt = cd_adlt.sortBy(lambda x: x[11], ascending=False)\n",
    "\n",
    "cd_eldery_2010_2019 = cd_aggr_2010_2019.filter(lambda x: x[11] > \"64\")\n",
    "cd_eldery_2020_ = cd_aggr_2020_.filter(lambda x: x[11] > \"64\")\n",
    "cd_eldery = cd_eldery_2010_2019.union(cd_eldery_2020_)\n",
    "sorted_cd_eldery = cd_eldery.sortBy(lambda x: x[11], ascending=False)\n",
    "\n",
    "end_time = time.time()\n",
    "time_ex=end_time - start_time\n",
    "\n",
    "print(\"Sample of sorted_cd_young_adlt:\")\n",
    "print(sorted_cd_young_adlt.take(10))\n",
    "\n",
    "print(\"Sample of sorted_cd_adlt:\")\n",
    "print(sorted_cd_adlt.take(10))\n",
    "\n",
    "print(\"Sample of sorted_cd_eldery:\")\n",
    "print(sorted_cd_eldery.take(10))\n",
    "\n",
    "# Take a sample of each RDD\n",
    "print(\"Sample of sorted_cd_kids:\")\n",
    "print(sorted_cd_kids.take(10))\n",
    "print(time_ex)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d91e948-7cef-4dd5-9a32-4f59051a20d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- dep_id: integer (nullable = true)\n",
      "\n",
      "+---+---------+------+------+\n",
      "| id|     name|salary|dep_id|\n",
      "+---+---------+------+------+\n",
      "|  6|  Jerry L| 550.0|     3|\n",
      "|  2|   John K|1000.0|     2|\n",
      "|  7| Marios K|1000.0|     1|\n",
      "|  5|  Helen K|1050.0|     2|\n",
      "| 10|Yiannis T|1500.0|     1|\n",
      "+---+---------+------+------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Spark DataFrame code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF query 1 execution\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "employees_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"salary\", FloatType()),\n",
    "    StructField(\"dep_id\", IntegerType()),\n",
    "])\n",
    "\n",
    "employees_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/employees.csv\", header=False, schema=employees_schema)\n",
    "#print the schema of the DataFrame:\n",
    "employees_df.printSchema()\n",
    "\n",
    "## Alternative way to read csv:\n",
    "# employees_df = spark.read.format('csv') \\\n",
    "#     .options(header='false') \\\n",
    "#     .schema(employees_schema) \\\n",
    "#     .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/spark-example-data/employees.csv\")\n",
    "\n",
    "sorted_employees_df = employees_df.sort(col(\"salary\"))\n",
    "sorted_employees_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b918f76f-656c-42a6-96bb-e15db0009797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUERY 2 WITH DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b400869-6bba-4061-9512-b634e92e9e83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|year|   precinct|  closed_case_rate|rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|    Rampart| 32.84713448949121|   1|\n",
      "|2010|    Olympic|31.515289821999087|   2|\n",
      "|2010|     Harbor| 29.36028339237341|   3|\n",
      "|2011|    Olympic|35.040060090135206|   1|\n",
      "|2011|    Rampart|  32.4964471814306|   2|\n",
      "|2011|     Harbor| 28.51336246316431|   3|\n",
      "|2012|    Olympic| 34.29708533302119|   1|\n",
      "|2012|    Rampart| 32.46000463714352|   2|\n",
      "|2012|     Harbor|29.509585848956675|   3|\n",
      "|2013|    Olympic| 33.58217940999398|   1|\n",
      "|2013|    Rampart|  32.1060382916053|   2|\n",
      "|2013|     Harbor|29.723638951488557|   3|\n",
      "|2014|   Van Nuys|  32.0215235281705|   1|\n",
      "|2014|West Valley| 31.49754809505847|   2|\n",
      "|2014|    Mission|31.224939855653567|   3|\n",
      "|2015|   Van Nuys|32.265140677157845|   1|\n",
      "|2015|    Mission|30.463762673676303|   2|\n",
      "|2015|   Foothill|30.353001803658852|   3|\n",
      "|2016|   Van Nuys|32.194518462124094|   1|\n",
      "|2016|West Valley| 31.40146437042384|   2|\n",
      "+----+-----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken by DataFrame API: 8.057204723358154 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "crime_df_2010_2019 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "crime_df_2020_present = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", header=True, inferSchema=True)\n",
    "# Ενοποίηση των δύο DataFrames\n",
    "\n",
    "crime_df = crime_df_2010_2019.union(crime_df_2020_present)\n",
    "crime_df = crime_df.select(\n",
    "    col(\"DATE OCC\").alias(\"date_occ\"),\n",
    "    col(\"AREA NAME\").alias(\"precinct\"),\n",
    "    col(\"Status Desc\").alias(\"status\")\n",
    ")\n",
    "crime_df = crime_df.withColumn(\"year\", col(\"date_occ\").substr(7, 4))\n",
    "crime_closed = crime_df.filter(col(\"status\").isin(\"Adult Arrest\", \"Adult Other\", \"Juv Arrest\", \"Juv Other\"))\n",
    "\n",
    "crime_stats = crime_closed.groupBy(\"year\", \"precinct\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"closed_cases\")\n",
    "    )\n",
    "total_cases = crime_df.groupBy(\"year\", \"precinct\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_cases\")\n",
    "    )\n",
    "crime_percentage = crime_stats.join(total_cases, [\"year\", \"precinct\"]) \\\n",
    "    .withColumn(\"closed_case_rate\", (col(\"closed_cases\") / col(\"total_cases\")) * 100)\n",
    "window_spec = Window.partitionBy(\"year\").orderBy(desc(\"closed_case_rate\"))\n",
    "crime_top3 = crime_percentage.withColumn(\"rank\", dense_rank().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") <= 3)\n",
    "\n",
    "crime_top3_sorted = crime_top3.orderBy(\"year\", \"rank\")\n",
    "\n",
    "start_time = time.time()\n",
    "crime_top3_sorted.select(\"year\", \"precinct\", \"closed_case_rate\", \"rank\").show()\n",
    "end_time = time.time()\n",
    "dataframe_api_time = end_time - start_time\n",
    "print(f\"Time taken by DataFrame API: {dataframe_api_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a253c45-fac7-4237-a253-aa5ef968a50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUERY 2 WITH SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "839fd1dc-6ebb-48f6-9a74-0e40e8ab105d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|year|   precinct|  closed_case_rate|rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|    Rampart| 32.84713448949121|   1|\n",
      "|2010|    Olympic|31.515289821999087|   2|\n",
      "|2010|     Harbor| 29.36028339237341|   3|\n",
      "|2011|    Olympic|35.040060090135206|   1|\n",
      "|2011|    Rampart|  32.4964471814306|   2|\n",
      "|2011|     Harbor| 28.51336246316431|   3|\n",
      "|2012|    Olympic| 34.29708533302119|   1|\n",
      "|2012|    Rampart| 32.46000463714352|   2|\n",
      "|2012|     Harbor|29.509585848956675|   3|\n",
      "|2013|    Olympic| 33.58217940999398|   1|\n",
      "|2013|    Rampart|  32.1060382916053|   2|\n",
      "|2013|     Harbor|29.723638951488557|   3|\n",
      "|2014|   Van Nuys|  32.0215235281705|   1|\n",
      "|2014|West Valley| 31.49754809505847|   2|\n",
      "|2014|    Mission|31.224939855653567|   3|\n",
      "|2015|   Van Nuys|32.265140677157845|   1|\n",
      "|2015|    Mission|30.463762673676303|   2|\n",
      "|2015|   Foothill|30.353001803658852|   3|\n",
      "|2016|   Van Nuys|32.194518462124094|   1|\n",
      "|2016|West Valley| 31.40146437042384|   2|\n",
      "+----+-----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken with SQL API : 8.786236047744751 seconds"
     ]
    }
   ],
   "source": [
    "# Register the DataFrames as temporary views\n",
    "crime_df.createOrReplaceTempView(\"crime_df\")\n",
    "crime_closed.createOrReplaceTempView(\"crime_closed\")\n",
    "crime_stats.createOrReplaceTempView(\"crime_stats\")\n",
    "total_cases.createOrReplaceTempView(\"total_cases\")\n",
    "crime_percentage.createOrReplaceTempView(\"crime_percentage\")\n",
    "crime_top3.createOrReplaceTempView(\"crime_top3\")\n",
    "\n",
    "\n",
    "sql_query = \"\"\"\n",
    "    WITH ranked_data AS (\n",
    "        SELECT\n",
    "            cp.year,\n",
    "            cp.precinct,\n",
    "            cp.closed_case_rate,\n",
    "            RANK() OVER (PARTITION BY cp.year ORDER BY cp.closed_case_rate DESC) AS rank\n",
    "        FROM\n",
    "            crime_percentage cp\n",
    "    )\n",
    "    SELECT\n",
    "        year,\n",
    "        precinct,\n",
    "        closed_case_rate,\n",
    "        rank\n",
    "    FROM\n",
    "        ranked_data\n",
    "    WHERE\n",
    "        rank <= 3\n",
    "    ORDER BY\n",
    "        year,\n",
    "        rank\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Execute the SQL query\n",
    "\n",
    "start_time_sql = time.time()\n",
    "crime_top3_sql = sc.sql(sql_query)\n",
    "crime_top3_sql.show()\n",
    "end_time_sql = time.time()\n",
    "\n",
    "sql_api_time = end_time_sql - start_time_sql\n",
    "print(f\"Time taken with SQL API : {sql_api_time} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aa59a3-ea33-400d-b5c2-add7fa68058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY 2 USING DATAFRAMES:  CSV VS PARQUET FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7910eb54-d414-4482-bdea-3f9dbbef4988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved in Parquet format to S3.\n",
      "Time taken with CSV input: 2.779564619064331 seconds\n",
      "Time taken with Parquet input: 0.4217860698699951 seconds\n",
      "CSV execution time: 2.779564619064331 seconds\n",
      "Parquet execution time: 0.4217860698699951 seconds\n",
      "Parquet was faster by: 2.357778549194336 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Δημιουργία Spark Session\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2 - CSV vs Parquet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "crime_df_2010_2019 = sc.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "crime_df_2020_present = sc.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Ενοποίηση των δεδομένων\n",
    "crime_df = crime_df_2010_2019.union(crime_df_2020_present)\n",
    "\n",
    "# Αποθήκευση σε μοναδικό Parquet αρχείο\n",
    "crime_df.write.mode(\"overwrite\").parquet(\"s3://groups-bucket-dblab-905418150721/group26/CrimeData.parquet\")\n",
    "\n",
    "print(\"Dataset saved in Parquet format to S3.\")\n",
    "\n",
    "# Χρονόμετρο για CSV\n",
    "start_time_csv = time.time()\n",
    "\n",
    "# Διαβάζουμε τα δεδομένα από CSV\n",
    "crime_df = sc.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True) \\\n",
    "    .union(sc.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", header=True, inferSchema=True))\n",
    "\n",
    "crime_df = crime_df.select(\n",
    "    col(\"DATE OCC\").alias(\"date_occ\"),\n",
    "    col(\"AREA NAME\").alias(\"precinct\"),\n",
    "    col(\"Status Desc\").alias(\"status\")\n",
    ")\n",
    "\n",
    "crime_df = crime_df.withColumn(\"year\", col(\"date_occ\").substr(7, 4))\n",
    "crime_closed = crime_df.filter(col(\"status\").isin(\"Adult Arrest\", \"Adult Other\", \"Juv Arrest\", \"Juv Other\"))\n",
    "\n",
    "crime_stats = crime_closed.groupBy(\"year\", \"precinct\") \\\n",
    "    .agg(count(\"*\").alias(\"closed_cases\"))\n",
    "total_cases = crime_df.groupBy(\"year\", \"precinct\") \\\n",
    "    .agg(count(\"*\").alias(\"total_cases\"))\n",
    "\n",
    "crime_percentage = crime_stats.join(total_cases, [\"year\", \"precinct\"]) \\\n",
    "    .withColumn(\"closed_case_rate\", (col(\"closed_cases\") / col(\"total_cases\")) * 100)\n",
    "\n",
    "window_spec = Window.partitionBy(\"year\").orderBy(desc(\"closed_case_rate\"))\n",
    "crime_top3 = crime_percentage.withColumn(\"rank\", dense_rank().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") <= 3)\n",
    "\n",
    "crime_top3_sorted = crime_top3.orderBy(\"year\", \"rank\")\n",
    "#crime_top3_sorted.show()\n",
    "\n",
    "end_time_csv = time.time()\n",
    "csv_time = end_time_csv - start_time_csv\n",
    "print(f\"Time taken with CSV input: {csv_time} seconds\")\n",
    "\n",
    "\n",
    "# Χρονόμετρο για Parquet\n",
    "start_time_parquet = time.time()\n",
    "\n",
    "# Διαβάζουμε τα δεδομένα από Parquet\n",
    "crime_df = sc.read.parquet(\"s3://groups-bucket-dblab-905418150721/group26/CrimeData.parquet\")\n",
    "\n",
    "crime_df = crime_df.select(\n",
    "    col(\"DATE OCC\").alias(\"date_occ\"),\n",
    "    col(\"AREA NAME\").alias(\"precinct\"),\n",
    "    col(\"Status Desc\").alias(\"status\")\n",
    ")\n",
    "\n",
    "crime_df = crime_df.withColumn(\"year\", col(\"date_occ\").substr(7, 4))\n",
    "crime_closed = crime_df.filter(col(\"status\").isin(\"Adult Arrest\", \"Adult Other\", \"Juv Arrest\", \"Juv Other\"))\n",
    "\n",
    "crime_stats = crime_closed.groupBy(\"year\", \"precinct\") \\\n",
    "    .agg(count(\"*\").alias(\"closed_cases\"))\n",
    "total_cases = crime_df.groupBy(\"year\", \"precinct\") \\\n",
    "    .agg(count(\"*\").alias(\"total_cases\"))\n",
    "\n",
    "crime_percentage = crime_stats.join(total_cases, [\"year\", \"precinct\"]) \\\n",
    "    .withColumn(\"closed_case_rate\", (col(\"closed_cases\") / col(\"total_cases\")) * 100)\n",
    "\n",
    "window_spec = Window.partitionBy(\"year\").orderBy(desc(\"closed_case_rate\"))\n",
    "crime_top3 = crime_percentage.withColumn(\"rank\", dense_rank().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") <= 3)\n",
    "\n",
    "crime_top3_sorted = crime_top3.orderBy(\"year\", \"rank\")\n",
    "#crime_top3_sorted.show()\n",
    "\n",
    "end_time_parquet = time.time()\n",
    "parquet_time = end_time_parquet - start_time_parquet\n",
    "print(f\"Time taken with Parquet input: {parquet_time} seconds\")\n",
    "\n",
    "\n",
    "print(f\"CSV execution time: {csv_time} seconds\")\n",
    "print(f\"Parquet execution time: {parquet_time} seconds\")\n",
    "print(f\"Parquet was faster by: {csv_time - parquet_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac62de-268a-4a84-ac20-6ee07525e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUERY 3 USING DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a9328593-9626-4c2e-9a1d-e1d85426d7b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------------+-------------------------+\n",
      "|                area|avg_income_per_person_per_area|avg_crime_rate_per_person|\n",
      "+--------------------+------------------------------+-------------------------+\n",
      "|     Adams-Normandie|             8791.458301453711|       1.0866524555163062|\n",
      "|              Alsace|             11239.50119372442|       1.8363603588772242|\n",
      "|Angeles National ...|            32657.048780487807|       14.861111111111112|\n",
      "|    Angelino Heights|            17552.282665401945|       3.5778648514508062|\n",
      "|              Arleta|            12113.216071836237|       0.6441891411956069|\n",
      "|     Atwater Village|            28481.236967160792|        2.234442935667145|\n",
      "|       Baldwin Hills|             17138.06555354499|        3.582901018138237|\n",
      "|             Bel Air|             63047.61900133641|        0.772383917544476|\n",
      "|       Beverly Crest|             58638.24134154688|       0.7292138407412925|\n",
      "|         Beverlywood|              29267.8210229561|       2.5187807537721465|\n",
      "|       Boyle Heights|              8641.17473859089|        2.103104505997807|\n",
      "|           Brentwood|              60849.0666249146|       0.8761558400454579|\n",
      "|           Brookside|            18001.811320754718|       1.0910848942447389|\n",
      "|    Cadillac-Corning|            19572.784696174043|         1.11721486051364|\n",
      "|         Canoga Park|            20126.459881328145|       2.7773106047295384|\n",
      "|             Carthay|             49808.45596918477|       2.7370714688421467|\n",
      "|             Central|             6971.210277672011|        3.216649845363862|\n",
      "|        Century City|             43793.96489803285|       11.208753532128794|\n",
      "|  Century Palms/Cove|             8492.414445490935|        5.450430903905403|\n",
      "|          Chatsworth|            30709.481240649096|        4.237860461380194|\n",
      "+--------------------+------------------------------+-------------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, regexp_replace, trim, coalesce, lit,explode, when\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Initialize SparkSession\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 3\") \\\n",
    "    .getOrCreate()\n",
    "############################################# AVERAGE INCOME PER PERSON PER AREA ######################################################\n",
    "\n",
    "\n",
    "######################################################   DATASET PREPROCESSING ########################################################\n",
    "\n",
    "##  INCOME DATASET ##\n",
    "\n",
    "income_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "income_df_cleaned = income_df.select(\n",
    "    col(\"Zip Code\").alias(\"income_zip\"),\n",
    "    regexp_replace(col(\"Estimated Median Income\"), \"[$,]\", \"\").cast(\"int\").alias(\"income\"),\n",
    "    col(\"Community\").alias(\"area_income\")  # Renaming area column to avoid ambiguity\n",
    ")\n",
    "\n",
    "#split the records that have multiple areas in the community \n",
    "income_split_df = income_df_cleaned.withColumn(\"area_income\", F.explode(split(col(\"area_income\"), r',\\s*|\\(\\s*|\\s*\\)')))\n",
    "income_split_df = income_split_df.filter(\n",
    "    (col(\"area_income\").isNotNull()) & (col(\"area_income\") != \"\")\n",
    ")\n",
    "# Trim whitespace from area names\n",
    "income_split_df = income_split_df.withColumn(\"area_income\", trim(col(\"area_income\")))\n",
    "income_split_df= income_split_df.orderBy(\"income_zip\")\n",
    "\n",
    "##  POPULATION DATASET  ##\n",
    "population_df = spark.read.option(\"multiline\", \"true\").json(\"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\")\n",
    "\n",
    "# Explode the 'features' array to access individual feature objects\n",
    "exploded_df = population_df.select(explode(col(\"features\")).alias(\"feature\"))\n",
    "\n",
    "cleaned_population_df = exploded_df.select(\n",
    "    col(\"feature.properties.COMM\").alias(\"area\"),\n",
    "    col(\"feature.properties.POP_2010\").alias(\"population\"),\n",
    "    col(\"feature.properties.ZCTA10\").alias(\"population_zip\"),\n",
    "    col(\"feature.properties.HOUSING10\").alias(\"households\"),\n",
    "    col(\"feature.properties.CITY\").alias(\"city\")\n",
    ")\n",
    "\n",
    "# Filter out rows with null or empty zip codes\n",
    "cleaned_population_df = cleaned_population_df.filter(\n",
    "    (col(\"city\") == \"Los Angeles\") &\n",
    "    (col(\"population_zip\").isNotNull()) & (col(\"population_zip\") != \"\") &\n",
    "    (col(\"households\").isNotNull()) & (col(\"households\") > 0) &\n",
    "     (col(\"population\").isNotNull()) & (col(\"population\") > 0)\n",
    "   \n",
    ")\n",
    "cleaned_population_df = cleaned_population_df.withColumn(\"area\", trim(col(\"area\")))\n",
    "cleaned_population_df = cleaned_population_df.filter(\n",
    "    (col(\"area\").isNotNull()) & (col(\"area\") != \"\")\n",
    ")\n",
    "#cleaned_population_df.show() #This dataframe contains the areas, population, zip and number of households per block\n",
    "\n",
    "\n",
    "####################################################### AVERAGE HOUSEHOLD SIZE PER AREA ##############################################\n",
    "\n",
    "#Now i will calculate the total population and total households for the areas with the same name and same zip code. So the total for every zip code./\n",
    "aggregated_population_df = cleaned_population_df.groupBy(\"area\", \"population_zip\").agg(\n",
    "    F.sum(\"population\").alias(\"total_population\"),\n",
    "    F.sum(\"households\").alias(\"total_households\")\n",
    ")\n",
    "\n",
    "#Now i will coacluate the average household size per zip\n",
    "aggregated_population_df = aggregated_population_df.withColumn(\n",
    "    \"avg_household_size_per_zip\",\n",
    "    when(col(\"total_households\") > 0, col(\"total_population\") / col(\"total_households\")).otherwise(None)\n",
    ")\n",
    "sorted_aggregated_population_df =  aggregated_population_df.orderBy(\"population_zip\")\n",
    "#sorted_aggregated_population_df.show()\n",
    "\n",
    "#Now i will calculate the average household size per area \n",
    "average_household_size_per_area = aggregated_population_df.groupBy(\"area\").agg(\n",
    "    F.sum((col(\"avg_household_size_per_zip\") * col(\"total_households\"))).alias(\"weighted_sum\"),\n",
    "    F.sum(\"total_households\").alias(\"total_households_per_area\")\n",
    ")\n",
    "\n",
    "average_household_size_per_area = average_household_size_per_area.withColumn(\n",
    "    \"avg_household_size_per_area\",\n",
    "    when(col(\"total_households_per_area\") > 0, col(\"weighted_sum\") / col(\"total_households_per_area\")).otherwise(None)\n",
    ")\n",
    "\n",
    "################################################### AVERAGE INCOME PER PERSON PER ZIP ################################################\n",
    "\n",
    "#For the same area, each zip code has its own mean income so I will first calculate the mean income per person for every zip\n",
    "\n",
    "joined_df = sorted_aggregated_population_df.join(\n",
    "    income_split_df,\n",
    "    (aggregated_population_df[\"population_zip\"] == income_split_df[\"income_zip\"]),\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "\n",
    "joined_df = joined_df.select(\n",
    "    \"area\", \"population_zip\", \"total_population\", \"total_households\",\n",
    "    \"avg_household_size_per_zip\", \"income\"\n",
    ").orderBy(\"area\", \"population_zip\")\n",
    "\n",
    "cleaned_joined_df = joined_df.filter(\n",
    "    (col(\"area\").isNotNull()) & (col(\"area\") != \"\")\n",
    ")\n",
    "cleaned_joined_df = cleaned_joined_df.filter(\n",
    "    (col(\"population_zip\").isNotNull()) & (col(\"population_zip\") != \"\")\n",
    ")\n",
    "#cleaned_joined_df.show()\n",
    "\n",
    "\n",
    "#Now i will calculate the average income per person per area \n",
    "joined_df = cleaned_joined_df.withColumn(\n",
    "    \"total_income_per_zip\",\n",
    "    col(\"income\") * col(\"total_households\")\n",
    ")\n",
    "\n",
    "aggregated_income_population_df = joined_df.groupBy(\"area\").agg(\n",
    "    F.sum(\"total_income_per_zip\").alias(\"total_income_area\"),\n",
    "    F.sum(\"total_population\").alias(\"total_population_area\")\n",
    ")\n",
    "\n",
    "aggregated_income_population_df = aggregated_income_population_df.withColumn(\n",
    "    \"avg_income_per_person_per_area\",\n",
    "    when(col(\"total_population_area\") > 0, col(\"total_income_area\") / col(\"total_population_area\")).otherwise(None)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################## CRIME RATE PER PERSON #####################################################\n",
    "\n",
    "######################################################   DATASET PREPROCESSING ########################################################\n",
    "import sedona\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeoJSON read\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create Sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# Read the GeoJSON file from S3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "# Formatting the GeoJSON data\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "\n",
    "# Print schema for the GeoJSON data\n",
    "#flattened_df.printSchema()\n",
    "\n",
    "# Read the Crime data files\n",
    "crime_df_2010_2019 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "crime_df_2020_present = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", header=True, inferSchema=True)\n",
    "crime_df = crime_df_2010_2019.union(crime_df_2020_present)\n",
    "\n",
    "# Clean the crime data to extract necessary columns (crime_id, LAT, LON)\n",
    "df1 = crime_df.select(\n",
    "    col(\"DR_NO\").alias(\"crime_id\"),  # Crime ID (DR_NO)\n",
    "    col(\"LAT\").alias(\"latitude\"),  # Latitude\n",
    "    col(\"LON\").alias(\"longitude\")  # Longitude\n",
    ")\n",
    "\n",
    "df1 = df1.withColumn(\"geom\", ST_Point(\"longitude\", \"latitude\"))\n",
    "\n",
    "joined_df1 = df1 \\\n",
    "    .join(flattened_df, ST_Within(df1.geom, flattened_df.geometry), \"inner\")\n",
    "\n",
    "\n",
    "final_df = joined_df1.select(\n",
    "    col(\"COMM\").alias(\"area\"),  # Area name (community)\n",
    "    col(\"POP_2010\").alias(\"pop10\"),  # Population of the block\n",
    "    col(\"geometry\"),  # Geometry of the block\n",
    "    col(\"latitude\").alias(\"latitude\"),  # Latitude of the crime location\n",
    "    col(\"longitude\").alias(\"longitude\"),  # Longitude of the crime location\n",
    "    col(\"crime_id\")  # Crime ID\n",
    ")\n",
    "#final_df.show()\n",
    "\n",
    "# Calculate the number of crimes per area\n",
    "crime_count_df = final_df.groupBy(\"area\", \"pop10\").agg(\n",
    "    count(\"crime_id\").alias(\"num_crimes\")\n",
    ")\n",
    "#This dataframe contain the average crime rate per person per block\n",
    "crime_rate_df = crime_count_df.withColumn(\n",
    "    \"crime_rate_per_person\",\n",
    "    col(\"num_crimes\") / col(\"pop10\")\n",
    ")\n",
    "filtered_df = crime_rate_df.filter(col(\"area\").isNotNull() & (col(\"area\") != \"\"))\n",
    "\n",
    "#I will calculate the crime rate per area per person, by finding the average.\n",
    "average_crime_rate_df = filtered_df.groupBy(\"area\").agg(\n",
    "    avg(\"crime_rate_per_person\").alias(\"avg_crime_rate_per_person\")\n",
    ")\n",
    "\n",
    "############################################# FINAL RESULTS INTO ONE TABLE ###########################################################\n",
    "\n",
    "final_crime_rate_per_area_per_person = average_crime_rate_df.orderBy(\"area\")\n",
    "\n",
    "\n",
    "final_result_df = aggregated_income_population_df.join(\n",
    "    final_crime_rate_per_area_per_person,\n",
    "    on=\"area\",\n",
    "    how=\"inner\" \n",
    ")\n",
    "\n",
    "final_result_df = final_result_df.select(\n",
    "    \"area\",\n",
    "    \"avg_income_per_person_per_area\",\n",
    "    \"avg_crime_rate_per_person\"\n",
    ")\n",
    "\n",
    "final_result_df.orderBy(\"area\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f689c135-597a-4164-af23-9ab6e6e8e5e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (22)\n",
      "+- Project (21)\n",
      "   +- Sort (20)\n",
      "      +- Exchange (19)\n",
      "         +- Project (18)\n",
      "            +- BroadcastHashJoin Inner BuildRight (17)\n",
      "               :- Project (9)\n",
      "               :  +- HashAggregate (8)\n",
      "               :     +- Exchange (7)\n",
      "               :        +- HashAggregate (6)\n",
      "               :           +- Project (5)\n",
      "               :              +- Filter (4)\n",
      "               :                 +- Generate (3)\n",
      "               :                    +- Filter (2)\n",
      "               :                       +- Scan json  (1)\n",
      "               +- BroadcastExchange (16)\n",
      "                  +- Project (15)\n",
      "                     +- Filter (14)\n",
      "                        +- Generate (13)\n",
      "                           +- Project (12)\n",
      "                              +- Filter (11)\n",
      "                                 +- Scan csv  (10)\n",
      "\n",
      "\n",
      "(1) Scan json \n",
      "Output [1]: [features#2157]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#2157]\n",
      "Condition : ((size(features#2157, true) > 0) AND isnotnull(features#2157))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#2157]\n",
      "Arguments: explode(features#2157), false, [feature#2165]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [feature#2165]\n",
      "Condition : (isnotnull(feature#2165.properties.CITY) AND ((((((((feature#2165.properties.CITY = Los Angeles) AND isnotnull(feature#2165.properties.ZCTA10)) AND NOT (feature#2165.properties.ZCTA10 = )) AND isnotnull(feature#2165.properties.HOUSING10)) AND (feature#2165.properties.HOUSING10 > 0)) AND isnotnull(feature#2165.properties.POP_2010)) AND (feature#2165.properties.POP_2010 > 0)) AND (isnotnull(trim(feature#2165.properties.COMM, None)) AND NOT (trim(feature#2165.properties.COMM, None) = ))))\n",
      "\n",
      "(5) Project\n",
      "Output [4]: [trim(feature#2165.properties.COMM, None) AS area#2183, feature#2165.properties.POP_2010 AS population#2169L, feature#2165.properties.ZCTA10 AS population_zip#2170, feature#2165.properties.HOUSING10 AS households#2171L]\n",
      "Input [1]: [feature#2165]\n",
      "\n",
      "(6) HashAggregate\n",
      "Input [4]: [area#2183, population#2169L, population_zip#2170, households#2171L]\n",
      "Keys [2]: [area#2183, population_zip#2170]\n",
      "Functions [2]: [partial_sum(population#2169L), partial_sum(households#2171L)]\n",
      "Aggregate Attributes [2]: [sum#2740L, sum#2742L]\n",
      "Results [4]: [area#2183, population_zip#2170, sum#2741L, sum#2743L]\n",
      "\n",
      "(7) Exchange\n",
      "Input [4]: [area#2183, population_zip#2170, sum#2741L, sum#2743L]\n",
      "Arguments: hashpartitioning(area#2183, population_zip#2170, 1000), ENSURE_REQUIREMENTS, [plan_id=2359]\n",
      "\n",
      "(8) HashAggregate\n",
      "Input [4]: [area#2183, population_zip#2170, sum#2741L, sum#2743L]\n",
      "Keys [2]: [area#2183, population_zip#2170]\n",
      "Functions [2]: [sum(population#2169L), sum(households#2171L)]\n",
      "Aggregate Attributes [2]: [sum(population#2169L)#2194L, sum(households#2171L)#2196L]\n",
      "Results [4]: [area#2183, population_zip#2170, sum(population#2169L)#2194L AS total_population#2195L, sum(households#2171L)#2196L AS total_households#2197L]\n",
      "\n",
      "(9) Project\n",
      "Output [5]: [area#2183, population_zip#2170, total_population#2195L, total_households#2197L, CASE WHEN (total_households#2197L > 0) THEN (cast(total_population#2195L as double) / cast(total_households#2197L as double)) END AS avg_household_size_per_zip#2202]\n",
      "Input [4]: [area#2183, population_zip#2170, total_population#2195L, total_households#2197L]\n",
      "\n",
      "(10) Scan csv \n",
      "Output [3]: [Zip Code#2134, Community#2135, Estimated Median Income#2136]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int,Community:string,Estimated Median Income:string>\n",
      "\n",
      "(11) Filter\n",
      "Input [3]: [Zip Code#2134, Community#2135, Estimated Median Income#2136]\n",
      "Condition : isnotnull(Zip Code#2134)\n",
      "\n",
      "(12) Project\n",
      "Output [3]: [Zip Code#2134 AS income_zip#2140, cast(regexp_replace(Estimated Median Income#2136, [$,], , 1) as int) AS income#2141, Community#2135 AS area_income#2142]\n",
      "Input [3]: [Zip Code#2134, Community#2135, Estimated Median Income#2136]\n",
      "\n",
      "(13) Generate\n",
      "Input [3]: [income_zip#2140, income#2141, area_income#2142]\n",
      "Arguments: explode(split(area_income#2142, ,\\s*|\\(\\s*|\\s*\\), -1)), [income_zip#2140, income#2141], false, [area_income#2148]\n",
      "\n",
      "(14) Filter\n",
      "Input [3]: [income_zip#2140, income#2141, area_income#2148]\n",
      "Condition : NOT (area_income#2148 = )\n",
      "\n",
      "(15) Project\n",
      "Output [2]: [income_zip#2140, income#2141]\n",
      "Input [3]: [income_zip#2140, income#2141, area_income#2148]\n",
      "\n",
      "(16) BroadcastExchange\n",
      "Input [2]: [income_zip#2140, income#2141]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2363]\n",
      "\n",
      "(17) BroadcastHashJoin\n",
      "Left keys [1]: [cast(population_zip#2170 as int)]\n",
      "Right keys [1]: [income_zip#2140]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(18) Project\n",
      "Output [6]: [area#2183, population_zip#2170, total_population#2195L, total_households#2197L, avg_household_size_per_zip#2202, income#2141]\n",
      "Input [7]: [area#2183, population_zip#2170, total_population#2195L, total_households#2197L, avg_household_size_per_zip#2202, income_zip#2140, income#2141]\n",
      "\n",
      "(19) Exchange\n",
      "Input [6]: [area#2183, population_zip#2170, total_population#2195L, total_households#2197L, avg_household_size_per_zip#2202, income#2141]\n",
      "Arguments: rangepartitioning(area#2183 ASC NULLS FIRST, population_zip#2170 ASC NULLS FIRST, 1000), ENSURE_REQUIREMENTS, [plan_id=2367]\n",
      "\n",
      "(20) Sort\n",
      "Input [6]: [area#2183, population_zip#2170, total_population#2195L, total_households#2197L, avg_household_size_per_zip#2202, income#2141]\n",
      "Arguments: [area#2183 ASC NULLS FIRST, population_zip#2170 ASC NULLS FIRST], true, 0\n",
      "\n",
      "(21) Project\n",
      "Output [7]: [area#2183, population_zip#2170, total_population#2195L, total_households#2197L, avg_household_size_per_zip#2202, income#2141, (cast(income#2141 as bigint) * total_households#2197L) AS total_income_per_zip#2247L]\n",
      "Input [6]: [area#2183, population_zip#2170, total_population#2195L, total_households#2197L, avg_household_size_per_zip#2202, income#2141]\n",
      "\n",
      "(22) AdaptiveSparkPlan\n",
      "Output [7]: [area#2183, population_zip#2170, total_population#2195L, total_households#2197L, avg_household_size_per_zip#2202, income#2141, total_income_per_zip#2247L]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "joined_df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff267402-5b7c-433c-b0c8-b88b957c966f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "RangeJoin (13)\n",
      ":- Union (7)\n",
      ":  :- Project (3)\n",
      ":  :  +- Filter (2)\n",
      ":  :     +- Scan csv  (1)\n",
      ":  +- Project (6)\n",
      ":     +- Filter (5)\n",
      ":        +- Scan csv  (4)\n",
      "+- * Project (12)\n",
      "   +- * Filter (11)\n",
      "      +- * Generate (10)\n",
      "         +- * Filter (9)\n",
      "            +- Scan geojson  (8)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [3]: [DR_NO#2451, LAT#2477, LON#2478]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<DR_NO:int,LAT:double,LON:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [3]: [DR_NO#2451, LAT#2477, LON#2478]\n",
      "Condition : isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "\n",
      "(3) Project\n",
      "Output [4]: [DR_NO#2451 AS crime_id#2609, LAT#2477 AS latitude#2610, LON#2478 AS longitude#2611,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#2615]\n",
      "Input [3]: [DR_NO#2451, LAT#2477, LON#2478]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [3]: [DR_NO#2525, LAT#2551, LON#2552]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "ReadSchema: struct<DR_NO:int,LAT:double,LON:double>\n",
      "\n",
      "(5) Filter\n",
      "Input [3]: [DR_NO#2525, LAT#2551, LON#2552]\n",
      "Condition : isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "\n",
      "(6) Project\n",
      "Output [4]: [DR_NO#2525 AS crime_id#2829, LAT#2551 AS latitude#2830, LON#2552 AS longitude#2831,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#2832]\n",
      "Input [3]: [DR_NO#2525, LAT#2551, LON#2552]\n",
      "\n",
      "(7) Union\n",
      "\n",
      "(8) Scan geojson \n",
      "Output [1]: [features#2288]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(9) Filter [codegen id : 1]\n",
      "Input [1]: [features#2288]\n",
      "Condition : ((size(features#2288, true) > 0) AND isnotnull(features#2288))\n",
      "\n",
      "(10) Generate [codegen id : 1]\n",
      "Input [1]: [features#2288]\n",
      "Arguments: explode(features#2288), false, [features#2296]\n",
      "\n",
      "(11) Filter [codegen id : 1]\n",
      "Input [1]: [features#2296]\n",
      "Condition : isnotnull(features#2296.geometry)\n",
      "\n",
      "(12) Project [codegen id : 1]\n",
      "Output [26]: [features#2296.properties.BG10 AS BG10#2305, features#2296.properties.BG10FIP10 AS BG10FIP10#2306, features#2296.properties.BG12 AS BG12#2307, features#2296.properties.CB10 AS CB10#2308, features#2296.properties.CEN_FIP13 AS CEN_FIP13#2309, features#2296.properties.CITY AS CITY#2310, features#2296.properties.CITYCOM AS CITYCOM#2311, features#2296.properties.COMM AS COMM#2312, features#2296.properties.CT10 AS CT10#2313, features#2296.properties.CT12 AS CT12#2314, features#2296.properties.CTCB10 AS CTCB10#2315, features#2296.properties.HD_2012 AS HD_2012#2316L, features#2296.properties.HD_NAME AS HD_NAME#2317, features#2296.properties.HOUSING10 AS HOUSING10#2318L, features#2296.properties.LA_FIP10 AS LA_FIP10#2319, features#2296.properties.OBJECTID AS OBJECTID#2320L, features#2296.properties.POP_2010 AS POP_2010#2321L, features#2296.properties.PUMA10 AS PUMA10#2322, features#2296.properties.SPA_2012 AS SPA_2012#2323L, features#2296.properties.SPA_NAME AS SPA_NAME#2324, features#2296.properties.SUP_DIST AS SUP_DIST#2325, features#2296.properties.SUP_LABEL AS SUP_LABEL#2326, features#2296.properties.ShapeSTArea AS ShapeSTArea#2327, features#2296.properties.ShapeSTLength AS ShapeSTLength#2328, features#2296.properties.ZCTA10 AS ZCTA10#2329, features#2296.geometry AS geometry#2299]\n",
      "Input [1]: [features#2296]\n",
      "\n",
      "(13) RangeJoin\n",
      "Arguments: geom#2615: geometry, geometry#2299: geometry, WITHIN"
     ]
    }
   ],
   "source": [
    "joined_df1.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97726be3-61a9-42d6-8c74-8d2feaf25427",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (46)\n",
      "+- Project (45)\n",
      "   +- SortMergeJoin Inner (44)\n",
      "      :- Sort (22)\n",
      "      :  +- Project (21)\n",
      "      :     +- HashAggregate (20)\n",
      "      :        +- Exchange (19)\n",
      "      :           +- HashAggregate (18)\n",
      "      :              +- Project (17)\n",
      "      :                 +- BroadcastHashJoin Inner BuildRight (16)\n",
      "      :                    :- HashAggregate (8)\n",
      "      :                    :  +- Exchange (7)\n",
      "      :                    :     +- HashAggregate (6)\n",
      "      :                    :        +- Project (5)\n",
      "      :                    :           +- Filter (4)\n",
      "      :                    :              +- Generate (3)\n",
      "      :                    :                 +- Filter (2)\n",
      "      :                    :                    +- Scan json  (1)\n",
      "      :                    +- BroadcastExchange (15)\n",
      "      :                       +- Project (14)\n",
      "      :                          +- Filter (13)\n",
      "      :                             +- Generate (12)\n",
      "      :                                +- Project (11)\n",
      "      :                                   +- Filter (10)\n",
      "      :                                      +- Scan csv  (9)\n",
      "      +- Sort (43)\n",
      "         +- HashAggregate (42)\n",
      "            +- Exchange (41)\n",
      "               +- HashAggregate (40)\n",
      "                  +- HashAggregate (39)\n",
      "                     +- Exchange (38)\n",
      "                        +- HashAggregate (37)\n",
      "                           +- Project (36)\n",
      "                              +- RangeJoin (35)\n",
      "                                 :- Union (29)\n",
      "                                 :  :- Project (25)\n",
      "                                 :  :  +- Filter (24)\n",
      "                                 :  :     +- Scan csv  (23)\n",
      "                                 :  +- Project (28)\n",
      "                                 :     +- Filter (27)\n",
      "                                 :        +- Scan csv  (26)\n",
      "                                 +- Project (34)\n",
      "                                    +- Filter (33)\n",
      "                                       +- Generate (32)\n",
      "                                          +- Filter (31)\n",
      "                                             +- Scan geojson  (30)\n",
      "\n",
      "\n",
      "(1) Scan json \n",
      "Output [1]: [features#2157]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#2157]\n",
      "Condition : ((size(features#2157, true) > 0) AND isnotnull(features#2157))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#2157]\n",
      "Arguments: explode(features#2157), false, [feature#2165]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [feature#2165]\n",
      "Condition : (isnotnull(feature#2165.properties.CITY) AND ((((((((feature#2165.properties.CITY = Los Angeles) AND isnotnull(feature#2165.properties.ZCTA10)) AND NOT (feature#2165.properties.ZCTA10 = )) AND isnotnull(feature#2165.properties.HOUSING10)) AND (feature#2165.properties.HOUSING10 > 0)) AND isnotnull(feature#2165.properties.POP_2010)) AND (feature#2165.properties.POP_2010 > 0)) AND (isnotnull(trim(feature#2165.properties.COMM, None)) AND NOT (trim(feature#2165.properties.COMM, None) = ))))\n",
      "\n",
      "(5) Project\n",
      "Output [4]: [trim(feature#2165.properties.COMM, None) AS area#2183, feature#2165.properties.POP_2010 AS population#2169L, feature#2165.properties.ZCTA10 AS population_zip#2170, feature#2165.properties.HOUSING10 AS households#2171L]\n",
      "Input [1]: [feature#2165]\n",
      "\n",
      "(6) HashAggregate\n",
      "Input [4]: [area#2183, population#2169L, population_zip#2170, households#2171L]\n",
      "Keys [2]: [area#2183, population_zip#2170]\n",
      "Functions [2]: [partial_sum(population#2169L), partial_sum(households#2171L)]\n",
      "Aggregate Attributes [2]: [sum#2740L, sum#2742L]\n",
      "Results [4]: [area#2183, population_zip#2170, sum#2741L, sum#2743L]\n",
      "\n",
      "(7) Exchange\n",
      "Input [4]: [area#2183, population_zip#2170, sum#2741L, sum#2743L]\n",
      "Arguments: hashpartitioning(area#2183, population_zip#2170, 1000), ENSURE_REQUIREMENTS, [plan_id=2701]\n",
      "\n",
      "(8) HashAggregate\n",
      "Input [4]: [area#2183, population_zip#2170, sum#2741L, sum#2743L]\n",
      "Keys [2]: [area#2183, population_zip#2170]\n",
      "Functions [2]: [sum(population#2169L), sum(households#2171L)]\n",
      "Aggregate Attributes [2]: [sum(population#2169L)#2194L, sum(households#2171L)#2196L]\n",
      "Results [4]: [area#2183, population_zip#2170, sum(population#2169L)#2194L AS total_population#2195L, sum(households#2171L)#2196L AS total_households#2197L]\n",
      "\n",
      "(9) Scan csv \n",
      "Output [3]: [Zip Code#2134, Community#2135, Estimated Median Income#2136]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:int,Community:string,Estimated Median Income:string>\n",
      "\n",
      "(10) Filter\n",
      "Input [3]: [Zip Code#2134, Community#2135, Estimated Median Income#2136]\n",
      "Condition : isnotnull(Zip Code#2134)\n",
      "\n",
      "(11) Project\n",
      "Output [3]: [Zip Code#2134 AS income_zip#2140, cast(regexp_replace(Estimated Median Income#2136, [$,], , 1) as int) AS income#2141, Community#2135 AS area_income#2142]\n",
      "Input [3]: [Zip Code#2134, Community#2135, Estimated Median Income#2136]\n",
      "\n",
      "(12) Generate\n",
      "Input [3]: [income_zip#2140, income#2141, area_income#2142]\n",
      "Arguments: explode(split(area_income#2142, ,\\s*|\\(\\s*|\\s*\\), -1)), [income_zip#2140, income#2141], false, [area_income#2148]\n",
      "\n",
      "(13) Filter\n",
      "Input [3]: [income_zip#2140, income#2141, area_income#2148]\n",
      "Condition : NOT (area_income#2148 = )\n",
      "\n",
      "(14) Project\n",
      "Output [2]: [income_zip#2140, income#2141]\n",
      "Input [3]: [income_zip#2140, income#2141, area_income#2148]\n",
      "\n",
      "(15) BroadcastExchange\n",
      "Input [2]: [income_zip#2140, income#2141]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2704]\n",
      "\n",
      "(16) BroadcastHashJoin\n",
      "Left keys [1]: [cast(population_zip#2170 as int)]\n",
      "Right keys [1]: [income_zip#2140]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(17) Project\n",
      "Output [3]: [area#2183, total_population#2195L, (cast(income#2141 as bigint) * total_households#2197L) AS total_income_per_zip#2247L]\n",
      "Input [6]: [area#2183, population_zip#2170, total_population#2195L, total_households#2197L, income_zip#2140, income#2141]\n",
      "\n",
      "(18) HashAggregate\n",
      "Input [3]: [area#2183, total_population#2195L, total_income_per_zip#2247L]\n",
      "Keys [1]: [area#2183]\n",
      "Functions [2]: [partial_sum(total_income_per_zip#2247L), partial_sum(total_population#2195L)]\n",
      "Aggregate Attributes [2]: [sum#2736L, sum#2738L]\n",
      "Results [3]: [area#2183, sum#2737L, sum#2739L]\n",
      "\n",
      "(19) Exchange\n",
      "Input [3]: [area#2183, sum#2737L, sum#2739L]\n",
      "Arguments: hashpartitioning(area#2183, 1000), ENSURE_REQUIREMENTS, [plan_id=2709]\n",
      "\n",
      "(20) HashAggregate\n",
      "Input [3]: [area#2183, sum#2737L, sum#2739L]\n",
      "Keys [1]: [area#2183]\n",
      "Functions [2]: [sum(total_income_per_zip#2247L), sum(total_population#2195L)]\n",
      "Aggregate Attributes [2]: [sum(total_income_per_zip#2247L)#2262L, sum(total_population#2195L)#2264L]\n",
      "Results [3]: [area#2183, sum(total_income_per_zip#2247L)#2262L AS total_income_area#2263L, sum(total_population#2195L)#2264L AS total_population_area#2265L]\n",
      "\n",
      "(21) Project\n",
      "Output [2]: [area#2183, CASE WHEN (total_population_area#2265L > 0) THEN (cast(total_income_area#2263L as double) / cast(total_population_area#2265L as double)) END AS avg_income_per_person_per_area#2269]\n",
      "Input [3]: [area#2183, total_income_area#2263L, total_population_area#2265L]\n",
      "\n",
      "(22) Sort\n",
      "Input [2]: [area#2183, avg_income_per_person_per_area#2269]\n",
      "Arguments: [area#2183 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(23) Scan csv \n",
      "Output [3]: [DR_NO#2451, LAT#2477, LON#2478]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<DR_NO:int,LAT:double,LON:double>\n",
      "\n",
      "(24) Filter\n",
      "Input [3]: [DR_NO#2451, LAT#2477, LON#2478]\n",
      "Condition : isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "\n",
      "(25) Project\n",
      "Output [2]: [DR_NO#2451 AS crime_id#2609,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#2615]\n",
      "Input [3]: [DR_NO#2451, LAT#2477, LON#2478]\n",
      "\n",
      "(26) Scan csv \n",
      "Output [3]: [DR_NO#2525, LAT#2551, LON#2552]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "ReadSchema: struct<DR_NO:int,LAT:double,LON:double>\n",
      "\n",
      "(27) Filter\n",
      "Input [3]: [DR_NO#2525, LAT#2551, LON#2552]\n",
      "Condition : isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "\n",
      "(28) Project\n",
      "Output [2]: [DR_NO#2525 AS crime_id#2833,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#2836]\n",
      "Input [3]: [DR_NO#2525, LAT#2551, LON#2552]\n",
      "\n",
      "(29) Union\n",
      "\n",
      "(30) Scan geojson \n",
      "Output [1]: [features#2288]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(31) Filter\n",
      "Input [1]: [features#2288]\n",
      "Condition : ((size(features#2288, true) > 0) AND isnotnull(features#2288))\n",
      "\n",
      "(32) Generate\n",
      "Input [1]: [features#2288]\n",
      "Arguments: explode(features#2288), false, [features#2296]\n",
      "\n",
      "(33) Filter\n",
      "Input [1]: [features#2296]\n",
      "Condition : (((isnotnull(features#2296.properties.COMM) AND NOT (features#2296.properties.COMM = )) AND isnotnull(features#2296.geometry)) AND bloomfilter#2837 of [bf2837 area#2183 estimatedNumRows=326225] filtering [features#2296.properties.COMM])\n",
      "\n",
      "(34) Project\n",
      "Output [3]: [features#2296.properties.COMM AS COMM#2312, features#2296.properties.POP_2010 AS POP_2010#2321L, features#2296.geometry AS geometry#2299]\n",
      "Input [1]: [features#2296]\n",
      "\n",
      "(35) RangeJoin\n",
      "Arguments: geom#2615: geometry, geometry#2299: geometry, WITHIN\n",
      "\n",
      "(36) Project\n",
      "Output [3]: [COMM#2312 AS area#2680, POP_2010#2321L AS pop10#2681L, crime_id#2609]\n",
      "Input [5]: [crime_id#2609, geom#2615, COMM#2312, POP_2010#2321L, geometry#2299]\n",
      "\n",
      "(37) HashAggregate\n",
      "Input [3]: [area#2680, pop10#2681L, crime_id#2609]\n",
      "Keys [2]: [area#2680, pop10#2681L]\n",
      "Functions [1]: [partial_count(crime_id#2609)]\n",
      "Aggregate Attributes [1]: [count#2748L]\n",
      "Results [3]: [area#2680, pop10#2681L, count#2749L]\n",
      "\n",
      "(38) Exchange\n",
      "Input [3]: [area#2680, pop10#2681L, count#2749L]\n",
      "Arguments: hashpartitioning(area#2680, pop10#2681L, 1000), ENSURE_REQUIREMENTS, [plan_id=2820]\n",
      "\n",
      "(39) HashAggregate\n",
      "Input [3]: [area#2680, pop10#2681L, count#2749L]\n",
      "Keys [2]: [area#2680, pop10#2681L]\n",
      "Functions [1]: [count(crime_id#2609)]\n",
      "Aggregate Attributes [1]: [count(crime_id#2609)#2696L]\n",
      "Results [2]: [area#2680, (cast(count(crime_id#2609)#2696L as double) / cast(pop10#2681L as double)) AS crime_rate_per_person#2701]\n",
      "\n",
      "(40) HashAggregate\n",
      "Input [2]: [area#2680, crime_rate_per_person#2701]\n",
      "Keys [1]: [area#2680]\n",
      "Functions [1]: [partial_avg(crime_rate_per_person#2701)]\n",
      "Aggregate Attributes [2]: [sum#2744, count#2745L]\n",
      "Results [3]: [area#2680, sum#2746, count#2747L]\n",
      "\n",
      "(41) Exchange\n",
      "Input [3]: [area#2680, sum#2746, count#2747L]\n",
      "Arguments: hashpartitioning(area#2680, 1000), ENSURE_REQUIREMENTS, [plan_id=2823]\n",
      "\n",
      "(42) HashAggregate\n",
      "Input [3]: [area#2680, sum#2746, count#2747L]\n",
      "Keys [1]: [area#2680]\n",
      "Functions [1]: [avg(crime_rate_per_person#2701)]\n",
      "Aggregate Attributes [1]: [avg(crime_rate_per_person#2701)#2710]\n",
      "Results [2]: [area#2680, avg(crime_rate_per_person#2701)#2710 AS avg_crime_rate_per_person#2711]\n",
      "\n",
      "(43) Sort\n",
      "Input [2]: [area#2680, avg_crime_rate_per_person#2711]\n",
      "Arguments: [area#2680 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(44) SortMergeJoin\n",
      "Left keys [1]: [area#2183]\n",
      "Right keys [1]: [area#2680]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(45) Project\n",
      "Output [3]: [area#2183, avg_income_per_person_per_area#2269, avg_crime_rate_per_person#2711]\n",
      "Input [4]: [area#2183, avg_income_per_person_per_area#2269, area#2680, avg_crime_rate_per_person#2711]\n",
      "\n",
      "(46) AdaptiveSparkPlan\n",
      "Output [3]: [area#2183, avg_income_per_person_per_area#2269, avg_crime_rate_per_person#2711]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "===== Subqueries =====\n",
      "\n",
      "Subquery:1 Hosting operator id = 33 Hosting Expression = bloomfilter#2837 of [bf2837 area#2183 estimatedNumRows=326225] filtering [features#2296.properties.COMM]\n",
      "OutputAdapter (55)\n",
      "+- AdaptiveSparkPlan (54)\n",
      "   +- Exchange (53)\n",
      "      +- HashAggregate (52)\n",
      "         +- Project (51)\n",
      "            +- Filter (50)\n",
      "               +- Generate (49)\n",
      "                  +- Filter (48)\n",
      "                     +- Scan json  (47)\n",
      "\n",
      "\n",
      "(47) Scan json \n",
      "Output [1]: [features#2157]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(48) Filter\n",
      "Input [1]: [features#2157]\n",
      "Condition : ((size(features#2157, true) > 0) AND isnotnull(features#2157))\n",
      "\n",
      "(49) Generate\n",
      "Input [1]: [features#2157]\n",
      "Arguments: explode(features#2157), false, [feature#2165]\n",
      "\n",
      "(50) Filter\n",
      "Input [1]: [feature#2165]\n",
      "Condition : (isnotnull(feature#2165.properties.CITY) AND ((((((((feature#2165.properties.CITY = Los Angeles) AND isnotnull(feature#2165.properties.ZCTA10)) AND NOT (feature#2165.properties.ZCTA10 = )) AND isnotnull(feature#2165.properties.HOUSING10)) AND (feature#2165.properties.HOUSING10 > 0)) AND isnotnull(feature#2165.properties.POP_2010)) AND (feature#2165.properties.POP_2010 > 0)) AND (isnotnull(trim(feature#2165.properties.COMM, None)) AND NOT (trim(feature#2165.properties.COMM, None) = ))))\n",
      "\n",
      "(51) Project\n",
      "Output [4]: [trim(feature#2165.properties.COMM, None) AS area#2183, feature#2165.properties.POP_2010 AS population#2169L, feature#2165.properties.ZCTA10 AS population_zip#2170, feature#2165.properties.HOUSING10 AS households#2171L]\n",
      "Input [1]: [feature#2165]\n",
      "\n",
      "(52) HashAggregate\n",
      "Input [4]: [area#2183, population#2169L, population_zip#2170, households#2171L]\n",
      "Keys [2]: [area#2183, population_zip#2170]\n",
      "Functions [2]: [partial_sum(population#2169L), partial_sum(households#2171L)]\n",
      "Aggregate Attributes [2]: [sum#2740L, sum#2742L]\n",
      "Results [4]: [area#2183, population_zip#2170, sum#2741L, sum#2743L]\n",
      "\n",
      "(53) Exchange\n",
      "Input [4]: [area#2183, population_zip#2170, sum#2741L, sum#2743L]\n",
      "Arguments: hashpartitioning(area#2183, population_zip#2170, 1000), ENSURE_REQUIREMENTS, [plan_id=2811]\n",
      "\n",
      "(54) AdaptiveSparkPlan\n",
      "Output [4]: [area#2183, population_zip#2170, sum#2741L, sum#2743L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "(55) OutputAdapter\n",
      "Output [4]: [area#2183, population_zip#2170, sum#2741L, sum#2743L]"
     ]
    }
   ],
   "source": [
    "final_result_df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0b92c177-d959-4227-95ff-6ec826dea1ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [cast(population_zip#772 as int)], [income_zip#742], Inner, BuildRight, false\n",
      "   :- Project [area#785, population_zip#772, total_population#797L, total_households#799L, CASE WHEN (total_households#799L > 0) THEN (cast(total_population#797L as double) / cast(total_households#799L as double)) END AS avg_household_size_per_zip#804]\n",
      "   :  +- HashAggregate(keys=[area#785, population_zip#772], functions=[sum(population#771L), sum(households#773L)], schema specialized)\n",
      "   :     +- Exchange hashpartitioning(area#785, population_zip#772, 1000), ENSURE_REQUIREMENTS, [plan_id=9773]\n",
      "   :        +- HashAggregate(keys=[area#785, population_zip#772], functions=[partial_sum(population#771L), partial_sum(households#773L)], schema specialized)\n",
      "   :           +- Project [trim(feature#767.properties.COMM, None) AS area#785, feature#767.properties.POP_2010 AS population#771L, feature#767.properties.ZCTA10 AS population_zip#772, feature#767.properties.HOUSING10 AS households#773L]\n",
      "   :              +- Filter (isnotnull(feature#767.properties.CITY) AND ((((((((feature#767.properties.CITY = Los Angeles) AND isnotnull(feature#767.properties.ZCTA10)) AND NOT (feature#767.properties.ZCTA10 = )) AND isnotnull(feature#767.properties.HOUSING10)) AND (feature#767.properties.HOUSING10 > 0)) AND isnotnull(feature#767.properties.POP_2010)) AND (feature#767.properties.POP_2010 > 0)) AND (isnotnull(trim(feature#767.properties.COMM, None)) AND NOT (trim(feature#767.properties.COMM, None) = ))))\n",
      "   :                 +- Generate explode(features#759), false, [feature#767]\n",
      "   :                    +- Filter ((size(features#759, true) > 0) AND isnotnull(features#759))\n",
      "   :                       +- FileScan json [features#759] Batched: false, DataFilters: [(size(features#759, true) > 0), isnotnull(features#759)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=9777]\n",
      "      +- Project [income_zip#742, income#743, trim(area_income#750, None) AS area_income#754]\n",
      "         +- Filter NOT (area_income#750 = )\n",
      "            +- Generate explode(split(area_income#744, ,\\s*|\\(\\s*|\\s*\\), -1)), [income_zip#742, income#743], false, [area_income#750]\n",
      "               +- Project [Zip Code#736 AS income_zip#742, cast(regexp_replace(Estimated Median Income#738, [$,], , 1) as int) AS income#743, Community#737 AS area_income#744]\n",
      "                  +- Filter isnotnull(Zip Code#736)\n",
      "                     +- FileScan csv [Zip Code#736,Community#737,Estimated Median Income#738] Batched: false, DataFilters: [isnotnull(Zip Code#736)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Community:string,Estimated Median Income:string>\n",
      "\n",
      "\n",
      "Method: broadcast | Execution Time: 9.1269 seconds | Row Count: 1212\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [cast(population_zip#772 as int)], [income_zip#742], Inner\n",
      "   :- Sort [cast(population_zip#772 as int) ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(cast(population_zip#772 as int), 1000), ENSURE_REQUIREMENTS, [plan_id=10165]\n",
      "   :     +- Project [area#785, population_zip#772, total_population#797L, total_households#799L, CASE WHEN (total_households#799L > 0) THEN (cast(total_population#797L as double) / cast(total_households#799L as double)) END AS avg_household_size_per_zip#804]\n",
      "   :        +- HashAggregate(keys=[area#785, population_zip#772], functions=[sum(population#771L), sum(households#773L)], schema specialized)\n",
      "   :           +- Exchange hashpartitioning(area#785, population_zip#772, 1000), ENSURE_REQUIREMENTS, [plan_id=10160]\n",
      "   :              +- HashAggregate(keys=[area#785, population_zip#772], functions=[partial_sum(population#771L), partial_sum(households#773L)], schema specialized)\n",
      "   :                 +- Project [trim(feature#767.properties.COMM, None) AS area#785, feature#767.properties.POP_2010 AS population#771L, feature#767.properties.ZCTA10 AS population_zip#772, feature#767.properties.HOUSING10 AS households#773L]\n",
      "   :                    +- Filter (isnotnull(feature#767.properties.CITY) AND ((((((((feature#767.properties.CITY = Los Angeles) AND isnotnull(feature#767.properties.ZCTA10)) AND NOT (feature#767.properties.ZCTA10 = )) AND isnotnull(feature#767.properties.HOUSING10)) AND (feature#767.properties.HOUSING10 > 0)) AND isnotnull(feature#767.properties.POP_2010)) AND (feature#767.properties.POP_2010 > 0)) AND (isnotnull(trim(feature#767.properties.COMM, None)) AND NOT (trim(feature#767.properties.COMM, None) = ))))\n",
      "   :                       +- Generate explode(features#759), false, [feature#767]\n",
      "   :                          +- Filter ((size(features#759, true) > 0) AND isnotnull(features#759))\n",
      "   :                             +- FileScan json [features#759] Batched: false, DataFilters: [(size(features#759, true) > 0), isnotnull(features#759)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "   +- Sort [income_zip#742 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(income_zip#742, 1000), ENSURE_REQUIREMENTS, [plan_id=10166]\n",
      "         +- Project [income_zip#742, income#743, trim(area_income#750, None) AS area_income#754]\n",
      "            +- Filter NOT (area_income#750 = )\n",
      "               +- Generate explode(split(area_income#744, ,\\s*|\\(\\s*|\\s*\\), -1)), [income_zip#742, income#743], false, [area_income#750]\n",
      "                  +- Project [Zip Code#736 AS income_zip#742, cast(regexp_replace(Estimated Median Income#738, [$,], , 1) as int) AS income#743, Community#737 AS area_income#744]\n",
      "                     +- Filter isnotnull(Zip Code#736)\n",
      "                        +- FileScan csv [Zip Code#736,Community#737,Estimated Median Income#738] Batched: false, DataFilters: [isnotnull(Zip Code#736)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Community:string,Estimated Median Income:string>\n",
      "\n",
      "\n",
      "Method: merge | Execution Time: 9.0542 seconds | Row Count: 1212\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- ShuffledHashJoin [cast(population_zip#772 as int)], [income_zip#742], Inner, BuildRight\n",
      "   :- Exchange hashpartitioning(cast(population_zip#772 as int), 1000), ENSURE_REQUIREMENTS, [plan_id=10678]\n",
      "   :  +- Project [area#785, population_zip#772, total_population#797L, total_households#799L, CASE WHEN (total_households#799L > 0) THEN (cast(total_population#797L as double) / cast(total_households#799L as double)) END AS avg_household_size_per_zip#804]\n",
      "   :     +- HashAggregate(keys=[area#785, population_zip#772], functions=[sum(population#771L), sum(households#773L)], schema specialized)\n",
      "   :        +- Exchange hashpartitioning(area#785, population_zip#772, 1000), ENSURE_REQUIREMENTS, [plan_id=10673]\n",
      "   :           +- HashAggregate(keys=[area#785, population_zip#772], functions=[partial_sum(population#771L), partial_sum(households#773L)], schema specialized)\n",
      "   :              +- Project [trim(feature#767.properties.COMM, None) AS area#785, feature#767.properties.POP_2010 AS population#771L, feature#767.properties.ZCTA10 AS population_zip#772, feature#767.properties.HOUSING10 AS households#773L]\n",
      "   :                 +- Filter (isnotnull(feature#767.properties.CITY) AND ((((((((feature#767.properties.CITY = Los Angeles) AND isnotnull(feature#767.properties.ZCTA10)) AND NOT (feature#767.properties.ZCTA10 = )) AND isnotnull(feature#767.properties.HOUSING10)) AND (feature#767.properties.HOUSING10 > 0)) AND isnotnull(feature#767.properties.POP_2010)) AND (feature#767.properties.POP_2010 > 0)) AND (isnotnull(trim(feature#767.properties.COMM, None)) AND NOT (trim(feature#767.properties.COMM, None) = ))))\n",
      "   :                    +- Generate explode(features#759), false, [feature#767]\n",
      "   :                       +- Filter ((size(features#759, true) > 0) AND isnotnull(features#759))\n",
      "   :                          +- FileScan json [features#759] Batched: false, DataFilters: [(size(features#759, true) > 0), isnotnull(features#759)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "   +- Exchange hashpartitioning(income_zip#742, 1000), ENSURE_REQUIREMENTS, [plan_id=10679]\n",
      "      +- Project [income_zip#742, income#743, trim(area_income#750, None) AS area_income#754]\n",
      "         +- Filter NOT (area_income#750 = )\n",
      "            +- Generate explode(split(area_income#744, ,\\s*|\\(\\s*|\\s*\\), -1)), [income_zip#742, income#743], false, [area_income#750]\n",
      "               +- Project [Zip Code#736 AS income_zip#742, cast(regexp_replace(Estimated Median Income#738, [$,], , 1) as int) AS income#743, Community#737 AS area_income#744]\n",
      "                  +- Filter isnotnull(Zip Code#736)\n",
      "                     +- FileScan csv [Zip Code#736,Community#737,Estimated Median Income#738] Batched: false, DataFilters: [isnotnull(Zip Code#736)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Community:string,Estimated Median Income:string>\n",
      "\n",
      "\n",
      "Method: shuffle_hash | Execution Time: 8.6711 seconds | Row Count: 1212\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- CartesianProduct (cast(population_zip#772 as int) = income_zip#742)\n",
      "   :- Project [area#785, population_zip#772, total_population#797L, total_households#799L, CASE WHEN (total_households#799L > 0) THEN (cast(total_population#797L as double) / cast(total_households#799L as double)) END AS avg_household_size_per_zip#804]\n",
      "   :  +- HashAggregate(keys=[area#785, population_zip#772], functions=[sum(population#771L), sum(households#773L)], schema specialized)\n",
      "   :     +- Exchange hashpartitioning(area#785, population_zip#772, 1000), ENSURE_REQUIREMENTS, [plan_id=11133]\n",
      "   :        +- HashAggregate(keys=[area#785, population_zip#772], functions=[partial_sum(population#771L), partial_sum(households#773L)], schema specialized)\n",
      "   :           +- Project [trim(feature#767.properties.COMM, None) AS area#785, feature#767.properties.POP_2010 AS population#771L, feature#767.properties.ZCTA10 AS population_zip#772, feature#767.properties.HOUSING10 AS households#773L]\n",
      "   :              +- Filter (isnotnull(feature#767.properties.CITY) AND ((((((((feature#767.properties.CITY = Los Angeles) AND isnotnull(feature#767.properties.ZCTA10)) AND NOT (feature#767.properties.ZCTA10 = )) AND isnotnull(feature#767.properties.HOUSING10)) AND (feature#767.properties.HOUSING10 > 0)) AND isnotnull(feature#767.properties.POP_2010)) AND (feature#767.properties.POP_2010 > 0)) AND (isnotnull(trim(feature#767.properties.COMM, None)) AND NOT (trim(feature#767.properties.COMM, None) = ))))\n",
      "   :                 +- Generate explode(features#759), false, [feature#767]\n",
      "   :                    +- Filter ((size(features#759, true) > 0) AND isnotnull(features#759))\n",
      "   :                       +- FileScan json [features#759] Batched: false, DataFilters: [(size(features#759, true) > 0), isnotnull(features#759)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>...\n",
      "   +- Project [income_zip#742, income#743, trim(area_income#750, None) AS area_income#754]\n",
      "      +- Filter NOT (area_income#750 = )\n",
      "         +- Generate explode(split(area_income#744, ,\\s*|\\(\\s*|\\s*\\), -1)), [income_zip#742, income#743], false, [area_income#750]\n",
      "            +- Project [Zip Code#736 AS income_zip#742, cast(regexp_replace(Estimated Median Income#738, [$,], , 1) as int) AS income#743, Community#737 AS area_income#744]\n",
      "               +- Filter isnotnull(Zip Code#736)\n",
      "                  +- FileScan csv [Zip Code#736,Community#737,Estimated Median Income#738] Batched: false, DataFilters: [isnotnull(Zip Code#736)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Community:string,Estimated Median Income:string>\n",
      "\n",
      "\n",
      "Method: shuffle_replicate_nl | Execution Time: 9.7411 seconds | Row Count: 1212"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "join_methods = [\"broadcast\", \"merge\", \"shuffle_hash\", \"shuffle_replicate_nl\"]\n",
    "results = []\n",
    "\n",
    "for method in join_methods:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    joined_df = sorted_aggregated_population_df.hint(method).join(\n",
    "        income_split_df.hint(method),\n",
    "        (sorted_aggregated_population_df[\"population_zip\"] == income_split_df[\"income_zip\"]),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    # Force execution and measure time\n",
    "    joined_df.explain()  # Show the execution plan\n",
    "    count = joined_df.count()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    results.append((\"Income-Population Join\", method, execution_time, count))\n",
    "    print(f\"Method: {method} | Execution Time: {execution_time:.4f} seconds | Row Count: {count}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04ea320-944e-49d2-b936-de543bc9ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUERY 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aceeb03e-7e69-47e3-a42e-c3a012db9881",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>369</td><td>application_1738075734771_0370</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0370/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-86.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0370_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '2g', 'spark.executor.cores': '1', 'spark.driver.memory': '2g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>270</td><td>application_1738075734771_0271</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0271/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-72.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0271_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>284</td><td>application_1738075734771_0285</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0285/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-137.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0285_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>285</td><td>application_1738075734771_0286</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0286/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0286_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>300</td><td>application_1738075734771_0301</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0301/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-241.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0301_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>326</td><td>application_1738075734771_0327</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0327/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-241.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0327_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>330</td><td>application_1738075734771_0331</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0331/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0331_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>332</td><td>application_1738075734771_0333</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0333/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-32.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0333_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>334</td><td>application_1738075734771_0335</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0335/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-75.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0335_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>343</td><td>application_1738075734771_0344</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0344/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0344_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>353</td><td>application_1738075734771_0354</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0354/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0354_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>368</td><td>application_1738075734771_0369</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0369/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0369_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>369</td><td>application_1738075734771_0370</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0370/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-86.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0370_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr><tr><td>370</td><td>application_1738075734771_0371</td><td>pyspark</td><td>starting</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0371/\">Link</a></td><td></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"1\",\n",
    "        \"spark.driver.memory\": \"2g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7039ebcc-1a82-4443-83cd-a15106d00a84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>371</td><td>application_1738075734771_0372</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0372/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-138.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0372_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '4g', 'spark.executor.cores': '2', 'spark.driver.memory': '2g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>270</td><td>application_1738075734771_0271</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0271/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-72.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0271_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>284</td><td>application_1738075734771_0285</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0285/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-137.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0285_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>285</td><td>application_1738075734771_0286</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0286/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0286_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>300</td><td>application_1738075734771_0301</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0301/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-241.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0301_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>326</td><td>application_1738075734771_0327</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0327/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-241.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0327_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>330</td><td>application_1738075734771_0331</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0331/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0331_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>332</td><td>application_1738075734771_0333</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0333/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-32.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0333_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>334</td><td>application_1738075734771_0335</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0335/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-75.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0335_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>343</td><td>application_1738075734771_0344</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0344/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0344_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>353</td><td>application_1738075734771_0354</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0354/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0354_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>368</td><td>application_1738075734771_0369</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0369/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0369_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>371</td><td>application_1738075734771_0372</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0372/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-138.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0372_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"4g\",\n",
    "        \"spark.executor.cores\": \"2\",\n",
    "        \"spark.driver.memory\": \"2g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "269247d9-7980-42db-ae27-c8c88bbcc326",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>373</td><td>application_1738075734771_0374</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0374/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-115.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0374_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '8g', 'spark.executor.cores': '4', 'spark.driver.memory': '2g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>270</td><td>application_1738075734771_0271</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0271/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-72.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0271_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>284</td><td>application_1738075734771_0285</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0285/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-137.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0285_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>285</td><td>application_1738075734771_0286</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0286/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0286_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>300</td><td>application_1738075734771_0301</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0301/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-241.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0301_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>326</td><td>application_1738075734771_0327</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0327/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-241.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0327_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>330</td><td>application_1738075734771_0331</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0331/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0331_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>332</td><td>application_1738075734771_0333</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0333/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-32.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0333_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>334</td><td>application_1738075734771_0335</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0335/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-75.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0335_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>343</td><td>application_1738075734771_0344</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0344/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0344_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>353</td><td>application_1738075734771_0354</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0354/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0354_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>368</td><td>application_1738075734771_0369</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0369/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0369_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>372</td><td>application_1738075734771_0373</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0373/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-115.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0373_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>373</td><td>application_1738075734771_0374</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0374/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-115.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0374_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"8g\",\n",
    "        \"spark.executor.cores\": \"4\",\n",
    "        \"spark.driver.memory\": \"2g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "759c0d26-3905-4d5d-bc56-628ac1eb1eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Areas by Income - Victim Descent Count:\n",
      "+--------------------+------------+\n",
      "|      Vict_Desc_Full|victim_count|\n",
      "+--------------------+------------+\n",
      "|               White|         695|\n",
      "|               Other|          86|\n",
      "|Hispanic/Latin/Me...|          77|\n",
      "|             Unknown|          49|\n",
      "|               Black|          43|\n",
      "|         Other Asian|          22|\n",
      "|             Chinese|           1|\n",
      "|American Indian/A...|           1|\n",
      "+--------------------+------------+\n",
      "\n",
      "Bottom 3 Areas by Income - Victim Descent Count:\n",
      "+--------------------+------------+\n",
      "|      Vict_Desc_Full|victim_count|\n",
      "+--------------------+------------+\n",
      "|Hispanic/Latin/Me...|        3215|\n",
      "|               Black|         872|\n",
      "|               White|         431|\n",
      "|               Other|         265|\n",
      "|         Other Asian|         142|\n",
      "|             Unknown|          26|\n",
      "|American Indian/A...|          24|\n",
      "|              Korean|           5|\n",
      "|             Chinese|           3|\n",
      "|            Filipino|           2|\n",
      "|         AsianIndian|           1|\n",
      "+--------------------+------------+\n",
      "\n",
      "Time taken: 59.17 seconds"
     ]
    }
   ],
   "source": [
    "# Query 4 with DataFrame\n",
    "import time\n",
    "import sedona\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import col, to_timestamp, year, count\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF query 4 execution\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "crimedata = StructType([\n",
    "    StructField(\"DR_NO\", StringType(), True),\n",
    "    StructField(\"Date Rptd\", StringType(), True),\n",
    "    StructField(\"DATE OCC\", StringType(), True),\n",
    "    StructField(\"TIME OCC\", StringType(), True),\n",
    "    StructField(\"AREA\", IntegerType(), True),\n",
    "    StructField(\"AREA NAME\", StringType(), True),\n",
    "    StructField(\"Rpt Dist No\", IntegerType(), True),\n",
    "    StructField(\"Part 1-2\", IntegerType(), True),\n",
    "    StructField(\"Crm Cd\", IntegerType(), True),\n",
    "    StructField(\"Crm Cd Desc\", StringType(), True),\n",
    "    StructField(\"Mocodes\", StringType(), True),\n",
    "    StructField(\"Vict Age\", IntegerType(), True),\n",
    "    StructField(\"Vict Sex\", StringType(), True),\n",
    "    StructField(\"Vict Descent\", StringType(), True),\n",
    "    StructField(\"Premis Cd\", IntegerType(), True),\n",
    "    StructField(\"Premis Desc\", StringType(), True),\n",
    "    StructField(\"Weapon Used Cd\", IntegerType(), True),\n",
    "    StructField(\"Weapon Desc\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Status Desc\", StringType(), True),\n",
    "    StructField(\"Crm Cd 1\", IntegerType(), True),\n",
    "    StructField(\"Crm Cd 2\", IntegerType(), True),\n",
    "    StructField(\"Crm Cd 3\", IntegerType(), True),\n",
    "    StructField(\"Crm Cd 4\", IntegerType(), True),\n",
    "    StructField(\"LOCATION\", StringType(), True),\n",
    "    StructField(\"Cross Street\", StringType(), True),\n",
    "    StructField(\"LAT\", FloatType(), True),\n",
    "    StructField(\"LON\", FloatType(), True)\n",
    "])\n",
    "\n",
    "REdata = StructType([\n",
    "    StructField(\"Vict_Desc\", StringType(), True),\n",
    "    StructField(\"Vict_Desc_Full\", StringType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "#we checked crime data from 2020 and on, and there were no incidents reported from 2015. Thus, we only used the one set\n",
    "crimedata_2010_2019 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=False, schema=crimedata)\n",
    "RE_codes = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=False, schema=REdata)\n",
    "\n",
    "crime_df_2015 = crimedata_2010_2019 \\\n",
    "    .withColumn(\"DATE OCC\", to_timestamp(col(\"DATE OCC\"), \"MM/dd/yyyy hh:mm:ss a\")) \\\n",
    "    .withColumn(\"DATE OCC\", year(col(\"DATE OCC\"))) \\\n",
    "    .filter(col(\"DATE OCC\") == 2015) \\\n",
    "    .filter(col(\"Vict Descent\").isNotNull())\n",
    "\n",
    "crime_df_2015 = crime_df_2015.select(\n",
    "    col(\"DATE OCC\").alias(\"date_occ\"),\n",
    "    col(\"AREA NAME\").alias(\"precinct\"),\n",
    "    col(\"Vict Descent\").alias(\"vict_descent\"),\n",
    "    col(\"DR_NO\").alias(\"crime_id\"),\n",
    "    col(\"LAT\").alias(\"latitude\"),\n",
    "    col(\"LON\").alias(\"longitude\")\n",
    ").withColumn(\"geom\", ST_Point(\"longitude\", \"latitude\"))\n",
    "\n",
    "# Top 3 areas by income excluding NULL and 0.0 values\n",
    "top_3_areas_by_income = aggregated_income_population_df.filter(\n",
    "    col(\"avg_income_per_person_per_area\").isNotNull() & (col(\"avg_income_per_person_per_area\") > 0.0)).orderBy(\n",
    "    col(\"avg_income_per_person_per_area\").desc()).limit(3)\n",
    "#top_3_areas_by_income.select(\"area\", \"avg_income_per_person_per_area\").show()\n",
    "\n",
    "# Bottom 3 areas by income excluding NULL and 0.0 values\n",
    "bottom_3_areas_by_income = aggregated_income_population_df.filter(\n",
    "    col(\"avg_income_per_person_per_area\").isNotNull() & (col(\"avg_income_per_person_per_area\") > 0.0)).orderBy(\n",
    "    col(\"avg_income_per_person_per_area\").asc()).limit(3)\n",
    "#bottom_3_areas_by_income.select(\"area\", \"avg_income_per_person_per_area\").show()\n",
    "\n",
    "joined_df_4 = crime_df_2015.join(flattened_df, ST_Within(crime_df_2015.geom, flattened_df.geometry), \"inner\")\n",
    "\n",
    "# Get a list of top and bottom areas\n",
    "top_areas = [row['area'] for row in top_3_areas_by_income.collect()]\n",
    "bottom_areas = [row['area'] for row in bottom_3_areas_by_income.collect()]\n",
    "\n",
    "final_df_4 = joined_df_4.select(\n",
    "    col(\"COMM\").alias(\"area\"),\n",
    "    col(\"precinct\").alias(\"precinct\"),\n",
    "    col(\"date_occ\").alias(\"date_occ\"),\n",
    "    col(\"geometry\"),\n",
    "    col(\"latitude\").alias(\"latitude\"),\n",
    "    col(\"longitude\").alias(\"longitude\"),\n",
    "    col(\"vict_descent\")  # Crime ID\n",
    ")\n",
    "\n",
    "crime_top_areas = final_df_4.filter(col(\"area\").isin(top_areas))\n",
    "# crime_top_areas.show()\n",
    "crime_bottom_areas = final_df_4.filter(col(\"area\").isin(bottom_areas))\n",
    "# crime_bottom_areas.show()\n",
    "\n",
    "# Join with RE_codes to get the full description\n",
    "crime_top_areas_with_re = crime_top_areas.join(RE_codes, crime_top_areas[\"vict_descent\"] == RE_codes[\"Vict_Desc\"], \"left\")\n",
    "crime_bottom_areas_with_re = crime_bottom_areas.join(RE_codes, crime_bottom_areas[\"vict_descent\"] == RE_codes[\"Vict_Desc\"], \"left\")\n",
    "#crime_top_areas_with_re.show(5)\n",
    "\n",
    "# Group by Victim Descent and count the number of victims\n",
    "top_areas_victim_count = crime_top_areas_with_re.groupBy(\"Vict_Desc_Full\").agg(count(\"*\").alias(\"victim_count\")).orderBy(col(\"victim_count\").desc())\n",
    "bottom_areas_victim_count = crime_bottom_areas_with_re.groupBy(\"Vict_Desc_Full\").agg(count(\"*\").alias(\"victim_count\")).orderBy(col(\"victim_count\").desc())\n",
    "\n",
    "# Show the results\n",
    "print(\"Top 3 Areas by Income - Victim Descent Count:\")\n",
    "top_areas_victim_count.show()\n",
    "\n",
    "print(\"Bottom 3 Areas by Income - Victim Descent Count:\")\n",
    "bottom_areas_victim_count.show()\n",
    "\n",
    "end_time = time.time()\n",
    "ex_time = end_time - start_time\n",
    "print(f\"Time taken: {ex_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "20ecc5ee-e73d-441a-a717-a739172fe284",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken when using 2 executors 1 core/2GB memory: 83.48 seconds"
     ]
    }
   ],
   "source": [
    "print(f\"Time taken when using 2 executors 1 core/2GB memory: {ex_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d6821177-2779-4b1f-b60c-0c4bd4a03892",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken when using 2 executors 2 core/4GB memory: 64.38 seconds"
     ]
    }
   ],
   "source": [
    "print(f\"Time taken when using 2 executors 2 core/4GB memory: {ex_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f2946fe7-1798-413d-8861-9221b4ec61a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken when using 2 executors 4 core/8GB memory: 59.17 seconds"
     ]
    }
   ],
   "source": [
    "print(f\"Time taken when using 2 executors 4 core/8GB memory: {ex_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0027bf8-ca8f-453c-990e-9f8faf83dead",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             area|\n",
      "+-----------------+\n",
      "|Manchester Square|\n",
      "|Mandeville Canyon|\n",
      "|        Mar Vista|\n",
      "| Marina Peninsula|\n",
      "|   Marina del Rey|\n",
      "|          Melrose|\n",
      "|         Mid-city|\n",
      "|     Miracle Mile|\n",
      "|    Mission Hills|\n",
      "|   Mt. Washington|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|             area|\n",
      "+-----------------+\n",
      "|Manchester Square|\n",
      "|Mandeville Canyon|\n",
      "|        Mar Vista|\n",
      "| Marina Peninsula|\n",
      "|          Melrose|\n",
      "|         Mid-city|\n",
      "|     Miracle Mile|\n",
      "|    Mission Hills|\n",
      "|   Mt. Washington|\n",
      "+-----------------+"
     ]
    }
   ],
   "source": [
    "final_df_4.select(\"area\").distinct() \\\n",
    "    .filter(col(\"area\").startswith(\"M\")) \\\n",
    "    .orderBy(\"area\") \\\n",
    "    .show()\n",
    "\n",
    "# Show distinct areas from cleaned_population_df that start with 'M', sorted by area\n",
    "cleaned_population_df.select(\"area\").distinct() \\\n",
    "    .filter(col(\"area\").startswith(\"M\")) \\\n",
    "    .orderBy(\"area\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1ebb634c-e57c-4620-bac7-4be17e1df5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 0.7246222496032715 seconds\n",
      "+---------+----------------+-----------+------------------+\n",
      "|police id|        division|crime_count|      avg_distance|\n",
      "+---------+----------------+-----------+------------------+\n",
      "|       13|       HOLLYWOOD|     224340|2.0762639601788018|\n",
      "|       16|        VAN NUYS|     210134|2.9533697428211574|\n",
      "|        5|       SOUTHWEST|     188901| 2.191398805778549|\n",
      "|       10|        WILSHIRE|     185996|2.5926655329791277|\n",
      "|        3|     77TH STREET|     171827|1.7165449719708101|\n",
      "|       11|         OLYMPIC|     170897|1.7236036971777815|\n",
      "|       15| NORTH HOLLYWOOD|     167854| 2.643006094141875|\n",
      "|        4|         PACIFIC|     161359| 3.850070655306162|\n",
      "|        7|         CENTRAL|     153871| 0.992476437456902|\n",
      "|       12|         RAMPART|     152736|1.5345341879190026|\n",
      "|        2|       SOUTHEAST|     152176| 2.421866215887735|\n",
      "|       17|     WEST VALLEY|     138643| 3.035671216314056|\n",
      "|       18|         TOPANGA|     138217|3.2969548417557832|\n",
      "|       19|        FOOTHILL|     134896|4.2509217084241815|\n",
      "|        1|          HARBOR|     126747|3.7025615993570953|\n",
      "|        9|      HOLLENBECK|     115837|2.6801812377068646|\n",
      "|        8|WEST LOS ANGELES|     115781|2.7924572890361095|\n",
      "|        6|          NEWTON|     111110|1.6346357397097262|\n",
      "|       14|       NORTHEAST|     108109|3.6236655246041196|\n",
      "|       21|         MISSION|     103355|3.6909426142788253|\n",
      "+---------+----------------+-----------+------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "##############################  QUERY 5 #################################\n",
    "import time\n",
    "from pyspark.sql.functions import format_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "start_time = time.time()\n",
    "police_stations = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\", header=True, inferSchema=True)\n",
    "police_stations = police_stations.withColumn('X', format_number('X', 10))  # Adjust the number of decimals as needed\n",
    "police_stations = police_stations.withColumn('Y', format_number('Y', 10))\n",
    "police_stations_df = police_stations.withColumn(\"geom\", ST_Point(col(\"X\"), col(\"Y\")))\n",
    "crime_df = df1.filter(\n",
    "    (F.col(\"longitude\") != 0) & (F.col(\"latitude\") != 0)\n",
    ")\n",
    "crime_df = crime_df.withColumnRenamed(\"geom\", \"crime_geom\")\n",
    "police_stations_df = police_stations_df.withColumnRenamed(\"geom\", \"police_geom\")\n",
    "\n",
    "joined_df = crime_df.crossJoin(police_stations_df)\n",
    "joined_df = joined_df.withColumn(\"distance\", ST_DistanceSphere(col(\"crime_geom\"), col(\"police_geom\"))/1000)\n",
    "joined_df = joined_df.select(\n",
    "     col(\"crime_id\").alias(\"crime_id\"),\n",
    "    col(\"distance\").alias(\"distance\"),\n",
    "    col(\"FID\").alias(\"police id\"),\n",
    "    col(\"DIVISION\").alias(\"division\")\n",
    ")\n",
    "\n",
    "joined_df = joined_df.orderBy(\"crime_id\")\n",
    "window_spec = Window.partitionBy(\"crime_id\").orderBy(\"distance\")\n",
    "df_with_rank = joined_df.withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "filtered_df = df_with_rank.filter(F.col(\"rank\") == 1)\n",
    "filtered_df = filtered_df.orderBy(\"police id\")\n",
    "crime_count_and_avg_distance = filtered_df.groupBy(\"police id\", \"division\").agg(\n",
    "    F.count(\"crime_id\").alias(\"crime_count\"),\n",
    "    F.avg(\"distance\").alias(\"avg_distance\")\n",
    ")\n",
    "crime_count_and_avg_distance_sorted = crime_count_and_avg_distance.orderBy(F.col(\"crime_count\").desc())\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution Time: {execution_time} seconds\")\n",
    "\n",
    "crime_count_and_avg_distance_sorted.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eaefcaee-278a-45ab-a09f-1a5dec4a67d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>30</td><td>application_1738075734771_0031</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0031/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0031_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '8g', 'spark.executor.cores': '4', 'spark.driver.memory': '2g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>11</td><td>application_1738075734771_0012</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0012/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-100.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0012_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>14</td><td>application_1738075734771_0015</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0015/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-139.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0015_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>15</td><td>application_1738075734771_0016</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0016/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0016_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>18</td><td>application_1738075734771_0019</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0019/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-137.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0019_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>21</td><td>application_1738075734771_0022</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0022/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-72.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0022_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>22</td><td>application_1738075734771_0023</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0023/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0023_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>26</td><td>application_1738075734771_0027</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0027/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-137.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0027_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>30</td><td>application_1738075734771_0031</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0031/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-194.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0031_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"8g\",\n",
    "        \"spark.executor.cores\": \"4\",\n",
    "        \"spark.driver.memory\": \"2g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe7111-37cc-4e6c-afd7-224b274377dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.memory\": \"4g\",\n",
    "        \"spark.executor.cores\": \"2\",\n",
    "        \"spark.driver.memory\": \"2g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94245f-9cbb-49f3-845f-4a1049126024",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"8\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"1\",\n",
    "        \"spark.driver.memory\": \"2g\"\n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
